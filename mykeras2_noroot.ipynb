{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using numpy 1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using keras 2.0.5\n",
      "[INFO] Using tensorflow 1.1.0\n",
      "[INFO] Using sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "import random\n",
    "random.seed(2023)\n",
    "print('[INFO] Using numpy {0}'.format(np.__version__))\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "old_stdout = sys.stdout\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Input, BatchNormalization\n",
    "from keras import initializers, regularizers, optimizers, losses\n",
    "#K.set_epsilon(1e-08)\n",
    "print('[INFO] Using keras {0}'.format(keras.__version__))\n",
    "\n",
    "import tensorflow as tf\n",
    "print('[INFO] Using tensorflow {0}'.format(tf.__version__))\n",
    "\n",
    "import sklearn\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "print('[INFO] Using sklearn {0}'.format(sklearn.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "print('[INFO] Current time {0}'.format(str(datetime.now())))\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Globals\n",
    "nlayers = 12  # 5 (CSC) + 4 (RPC) + 3 (GEM)\n",
    "\n",
    "nvariables = (nlayers * 5) + 8\n",
    "\n",
    "discr_pt_cut = 14.\n",
    "\n",
    "reg_pt_scale = 100.\n",
    "\n",
    "discr_loss_weight = 1.\n",
    "\n",
    "add_noise = True\n",
    "\n",
    "infile_muon = '/scratch/CMS/L1MuonTrigger/P2_9_2_3_patch1/SingleMuon_Toy_2GeV/histos_tba.13.npz'\n",
    "\n",
    "infile_pileup = '/scratch/CMS/L1MuonTrigger/P2_9_2_3_patch1/SingleMuon_Toy_2GeV/histos_tbd.13.npz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(object):\n",
    "\n",
    "  def __init__(self, x, y, adjust_scale=0):\n",
    "    if x is not None and y is not None:\n",
    "      assert(x.shape[1] == (nlayers * 6) + 4)\n",
    "      assert(y.shape[1] == 3)\n",
    "      assert(x.shape[0] == y.shape[0])\n",
    "\n",
    "      self.nentries = x.shape[0]\n",
    "      self.x_orig  = x\n",
    "      self.y_orig  = y\n",
    "      self.x_copy  = x.copy()\n",
    "      self.y_copy  = y.copy()\n",
    "\n",
    "      # Get views\n",
    "      self.x_phi   = self.x_copy[:, nlayers*0:nlayers*1]\n",
    "      self.x_theta = self.x_copy[:, nlayers*1:nlayers*2]\n",
    "      self.x_bend  = self.x_copy[:, nlayers*2:nlayers*3]\n",
    "      self.x_ring  = self.x_copy[:, nlayers*3:nlayers*4]\n",
    "      self.x_fr    = self.x_copy[:, nlayers*4:nlayers*5]\n",
    "      self.x_mask  = self.x_copy[:, nlayers*5:nlayers*6].astype(np.bool)  # this makes a copy\n",
    "      self.x_road  = self.x_copy[:, nlayers*6:nlayers*7]  # ipt, ieta, iphi, iphi_corr\n",
    "      self.y_pt    = self.y_copy[:, 0]  # q/pT\n",
    "      self.y_phi   = self.y_copy[:, 1]\n",
    "      self.y_eta   = self.y_copy[:, 2]\n",
    "      \n",
    "      # Make event weight\n",
    "      #self.w       = np.ones(self.y_pt.shape, dtype=np.float32)\n",
    "      self.w       = np.abs(self.y_pt)/0.2 + 1.0\n",
    "      \n",
    "      # Straightness & zone\n",
    "      self.x_straightness = self.x_road[:, 0][:, np.newaxis]\n",
    "      self.x_zone         = self.x_road[:, 1][:, np.newaxis]\n",
    "      \n",
    "      # Subtract median phi from hit phis\n",
    "      #self.x_phi_median    = self.x_road[:, 2] * 32 - 16  # multiply by 'quadstrip' unit (4 * 8)\n",
    "      self.x_phi_median    = self.x_road[:, 2] * 16 - 8  # multiply by 'doublestrip' unit (2 * 8)\n",
    "      self.x_phi_median    = self.x_phi_median[:, np.newaxis]\n",
    "      self.x_phi          -= self.x_phi_median\n",
    "      \n",
    "      # Subtract median theta from hit thetas\n",
    "      self.x_theta_median  = np.nanmedian(self.x_theta[:,:5], axis=1)  # CSC only\n",
    "      self.x_theta_median[np.isnan(self.x_theta_median)] = np.nanmedian(self.x_theta[np.isnan(self.x_theta_median)], axis=1)  # use all\n",
    "      self.x_theta_median  = self.x_theta_median[:, np.newaxis]\n",
    "      self.x_theta        -= self.x_theta_median\n",
    "      \n",
    "      # Standard scales\n",
    "      # + Remove outlier hits by checking hit thetas\n",
    "      if adjust_scale == 0:  # do not adjust\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 10000.0\n",
    "      elif adjust_scale == 1:  # use mean and std\n",
    "        self.x_mean  = np.nanmean(self.x_copy, axis=0)\n",
    "        self.x_std   = np.nanstd(self.x_copy, axis=0)\n",
    "        self.x_std   = self._handle_zero_in_scale(self.x_std)\n",
    "        self.x_copy -= self.x_mean\n",
    "        self.x_copy /= self.x_std\n",
    "        x_theta_tmp = np.abs(self.x_theta) > 1.0\n",
    "      elif adjust_scale == 2:  # adjust by hand\n",
    "        theta_cuts    = np.array((6., 6., 6., 6., 6., 12., 12., 12., 12., 9., 9., 9.), dtype=np.float32)\n",
    "        x_theta_tmp   = np.abs(self.x_theta) > theta_cuts\n",
    "        self.x_phi   *= 0.000991  # GE1/1 dphi linear correlation with q/pT\n",
    "        self.x_theta *= (1/12.)   # 12 integer theta units\n",
    "        self.x_bend  *= 0.188082  # ME1/2 bend linear correlation with q/pT\n",
    "        x_ring_tmp    = self.x_ring.astype(np.int32)\n",
    "        x_ring_tmp    = (x_ring_tmp == 1) | (x_ring_tmp == 4)\n",
    "        self.x_ring[x_ring_tmp] = 0  # ring 1,4 -> 0\n",
    "        self.x_ring[~x_ring_tmp] = 1 # ring 2,3 -> 1\n",
    "        x_fr_tmp      = self.x_fr.astype(np.int32)\n",
    "        x_fr_tmp      = (x_fr_tmp == 0)\n",
    "        self.x_fr[x_fr_tmp] = 0\n",
    "        self.x_fr[~x_fr_tmp] = 1\n",
    "      elif adjust_scale == 3:  # adjust by hand #2\n",
    "        #theta_cuts    = np.array((6., 6., 6., 6., 6., 12., 12., 12., 12., 9., 9., 9.), dtype=np.float32)\n",
    "        theta_cuts    = np.array((6., 6., 6., 6., 6., 10., 10., 10., 10., 8., 8., 8.), dtype=np.float32)\n",
    "        x_theta_tmp   = np.abs(self.x_theta) > theta_cuts\n",
    "        x_ring_tmp    = self.x_ring.astype(np.int32)\n",
    "        x_ring_tmp    = (x_ring_tmp == 1) | (x_ring_tmp == 4)\n",
    "        self.x_ring[x_ring_tmp] = 0  # ring 1,4 -> 0\n",
    "        self.x_ring[~x_ring_tmp] = 1 # ring 2,3 -> 1\n",
    "        x_fr_tmp      = self.x_fr.astype(np.int32)\n",
    "        x_fr_tmp      = (x_fr_tmp == 0)\n",
    "        self.x_fr[x_fr_tmp] = 0\n",
    "        self.x_fr[~x_fr_tmp] = 1\n",
    "        s = [ 0.00528005,  0.01100854, -0.01955833, -0.01326062, -0.00839341,\n",
    "              0.01209313, -0.02546741, -0.011541  , -0.00734255,  0.00393156,\n",
    "             -0.02459449,  1.        ,  0.55500895,  0.50743203,  1.4219028 ,\n",
    "              1.35162982,  0.93576706,  0.19965793,  0.29495697,  0.35250728,\n",
    "              0.38013349,  0.50885451,  0.66930139,  1.        ,  0.81924683,\n",
    "              0.47289819,  1.67281557,  1.1339659 ,  1.13266964,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,\n",
    "              1.        ,  1.        ,  1.        ,  1.        ,  1.        ,  1.        ]\n",
    "        self.x_copy *= s\n",
    "      \n",
    "      # Remove outlier hits by checking hit thetas\n",
    "      self.x_phi  [x_theta_tmp] = np.nan\n",
    "      self.x_theta[x_theta_tmp] = np.nan\n",
    "      self.x_bend [x_theta_tmp] = np.nan\n",
    "      self.x_ring [x_theta_tmp] = np.nan\n",
    "      self.x_fr   [x_theta_tmp] = np.nan\n",
    "      self.x_mask [x_theta_tmp] = 1.0\n",
    "      \n",
    "      # Add variables: straightness, zone, theta_median and mode variables\n",
    "      self.x_straightness = np.abs(self.x_straightness - 6.) / 6.  # scaled to [0,1]\n",
    "      self.x_zone         = (self.x_zone - 0.) / 5.  # scaled to [0,1]\n",
    "      self.x_theta_median = (self.x_theta_median - 3.) / 83.  # scaled to [0,1]\n",
    "      hits_to_station = np.array((5,1,2,3,4,1,2,3,4,5,2,5), dtype=np.int32)  # '5' denotes ME1/1\n",
    "      assert(len(hits_to_station) == nlayers)\n",
    "      self.x_mode_vars = np.zeros((self.nentries, 5), dtype=np.bool)\n",
    "      self.x_mode_vars[:,0] = np.any(self.x_mask[:,hits_to_station == 5] == 0, axis=1)\n",
    "      self.x_mode_vars[:,1] = np.any(self.x_mask[:,hits_to_station == 1] == 0, axis=1)\n",
    "      self.x_mode_vars[:,2] = np.any(self.x_mask[:,hits_to_station == 2] == 0, axis=1)\n",
    "      self.x_mode_vars[:,3] = np.any(self.x_mask[:,hits_to_station == 3] == 0, axis=1)\n",
    "      self.x_mode_vars[:,4] = np.any(self.x_mask[:,hits_to_station == 4] == 0, axis=1)\n",
    "      \n",
    "      # Remove NaN\n",
    "      #np.nan_to_num(self.x_copy, copy=False)\n",
    "      self.x_copy[np.isnan(self.x_copy)] = 0.0\n",
    "\n",
    "  # Copied from scikit-learn\n",
    "  def _handle_zero_in_scale(self, scale):\n",
    "    scale[scale == 0.0] = 1.0\n",
    "    return scale\n",
    "\n",
    "  def get_x(self):\n",
    "    #x_new = self.x_phi\n",
    "    x_new = np.hstack((self.x_phi, self.x_theta, self.x_bend, self.x_ring, self.x_fr, self.x_straightness, self.x_zone, self.x_theta_median, self.x_mode_vars))\n",
    "    return x_new\n",
    "\n",
    "  def get_x_mask(self):\n",
    "    x_mask = self.x_mask.copy()\n",
    "    return x_mask\n",
    "\n",
    "  def get_y(self):\n",
    "    y_new = self.y_pt.copy()\n",
    "    return y_new\n",
    "\n",
    "  def get_w(self):\n",
    "    w_new = self.w.copy()\n",
    "    return w_new\n",
    "\n",
    "  def save_encoder(self, filepath):\n",
    "    np.savez_compressed(filepath, x_mean=self.x_mean, x_std=self.x_std)\n",
    "\n",
    "  def load_endcoder(self, filepath):\n",
    "    loaded = np.load(filepath)\n",
    "    self.x_mean = loaded['x_mean']\n",
    "    self.x_std = loaded['x_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# New leaky relu\n",
    "def NewLeakyReLU(x, alpha=0., max_value=None):\n",
    "  return K.relu(x, alpha=alpha, max_value=max_value)\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# New tanh\n",
    "def NewTanh(x):\n",
    "  return K.tanh(x)\n",
    "  #return 1.7159 * K.tanh(x * 2./3.)\n",
    "  #return K.clip(x, -1., 1.)\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Huber loss\n",
    "def huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "def masked_huber_loss(y_true, y_pred, delta=1.345):\n",
    "  x = K.abs(y_true - y_pred)\n",
    "  squared_loss = 0.5*K.square(x)\n",
    "  absolute_loss = delta * (x - 0.5*delta)\n",
    "  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "\n",
    "  mask_value = 100.\n",
    "  mask = K.not_equal(y_true, mask_value)\n",
    "  mask = K.cast(mask, K.floatx())\n",
    "  xx *= mask\n",
    "  xx /= K.mean(mask)\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "#def masked_huber_loss(y_true, y_pred, delta=1.345):\n",
    "#  mask_value = 100.\n",
    "#  mask_alpha = 0.02\n",
    "#  mask_target = 0.5 * reg_pt_scale\n",
    "#  mask = K.equal(y_true, mask_value)\n",
    "#  \n",
    "#  #x = K.abs(y_true - y_pred)\n",
    "#  x = tf.where(mask, mask_alpha * K.abs(mask_target - K.abs(y_pred)), K.abs(y_true - y_pred))\n",
    "#  squared_loss = 0.5*K.square(x)\n",
    "#  absolute_loss = delta * (x - 0.5*delta)\n",
    "#  #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "#  xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "#  return K.mean(xx, axis=-1)\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Binary crossentropy\n",
    "def masked_binary_crossentropy(y_true, y_pred, from_logits=False):\n",
    "  target, output = y_true, y_pred\n",
    "\n",
    "  # transform back to logits\n",
    "  if not from_logits:\n",
    "    output = K.clip(output, K.epsilon(), 1 - K.epsilon())\n",
    "    output = K.log(output / (1 - output))\n",
    "  \n",
    "  xx =  tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "  #xx =  tf.nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=0.5)  # pos_weight < 1 decreases the false positive count\n",
    "\n",
    "  mask_value = 100.\n",
    "  mask = K.not_equal(y_true, mask_value)\n",
    "  mask = K.cast(mask, K.floatx())\n",
    "  xx *= mask\n",
    "  xx /= K.mean(mask)\n",
    "  return K.mean(xx, axis=-1)\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Learning rate decay by epoch number\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "  if (epoch % 10) == 0:\n",
    "    lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, lr*0.95)\n",
    "    print(\"lr changed to {}\".format(lr*0.95))\n",
    "  return K.get_value(model.optimizer.lr)\n",
    "\n",
    "lr_decay = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# Custom objects\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "get_custom_objects().update({'masked_huber_loss': masked_huber_loss, 'masked_binary_crossentropy': masked_binary_crossentropy, 'NewLeakyReLU': NewLeakyReLU, 'NewTanh': NewTanh})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading muon data ...\n",
      "[INFO] Loaded the variables with shape (3643811, 76)\n",
      "[INFO] Loaded the parameters with shape (3643811, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-numpy/1.12.1-mlhled2/lib/python2.7/site-packages/numpy-1.12.1-py2.7-linux-x86_64.egg/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "#### Load data ####\n",
    "\n",
    "def muon_data():\n",
    "  try:\n",
    "    print('[INFO] Loading muon data ...')\n",
    "    loaded = np.load(infile_muon)\n",
    "    the_variables = loaded['variables']\n",
    "    the_parameters = loaded['parameters']\n",
    "    print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "    print('[INFO] Loaded the parameters with shape {0}'.format(the_parameters.shape))\n",
    "  except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile_muon))\n",
    "\n",
    "  encoder = Encoder(the_variables, the_parameters, adjust_scale=3)\n",
    "  x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "  assert np.isfinite(x).all()\n",
    "\n",
    "  # Split dataset in training and testing\n",
    "  x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = train_test_split(x, y, w, x_mask, test_size=0.3)\n",
    "  return x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "x_train, x_test, y_train, y_test, w_train, w_test, x_mask_train, x_mask_test = muon_data()\n",
    "\n",
    "# Add output nodes\n",
    "labels = np.where(np.abs(1.0/y_train) > discr_pt_cut, 1., 100.)  # mask_value is set to 100\n",
    "y_train = [y_train, labels.astype(np.float32)]\n",
    "\n",
    "labels = np.where(np.abs(1.0/y_test) > discr_pt_cut, 1., 100.)  # mask_value is set to 100\n",
    "y_test = [y_test, labels.astype(np.float32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading pileup data ...\n",
      "[INFO] Loaded the variables with shape (171550, 76)\n",
      "[INFO] Loaded the auxiliary info with shape (171550, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/cms.cern.ch/slc6_amd64_gcc630/external/py2-pippkgs/5.0-ghjeda6/lib/python2.7/site-packages/ipykernel_launcher.py:75: RuntimeWarning: invalid value encountered in greater\n"
     ]
    }
   ],
   "source": [
    "#### Load data (pileup) ####\n",
    "\n",
    "def pileup_data():\n",
    "  try:\n",
    "    print('[INFO] Loading pileup data ...')\n",
    "    loaded = np.load(infile_pileup)\n",
    "    the_variables = loaded['variables']\n",
    "    the_parameters = np.zeros((the_variables.shape[0], 3), dtype=np.float32)\n",
    "    the_auxiliaries = loaded['aux']\n",
    "    print('[INFO] Loaded the variables with shape {0}'.format(the_variables.shape))\n",
    "    print('[INFO] Loaded the auxiliary info with shape {0}'.format(the_auxiliaries.shape))\n",
    "  except:\n",
    "    print('[ERROR] Failed to load data from file: {0}'.format(infile_pileup))\n",
    "\n",
    "  sel = the_auxiliaries[:,2] > discr_pt_cut\n",
    "  the_variables = the_variables[~sel]\n",
    "  the_parameters = the_parameters[~sel]\n",
    "  the_auxiliaries = the_auxiliaries[~sel]\n",
    "\n",
    "  encoder = Encoder(the_variables, the_parameters, adjust_scale=3)\n",
    "  x, y, w, x_mask = encoder.get_x(), encoder.get_y(), encoder.get_w(), encoder.get_x_mask()\n",
    "  aux = the_auxiliaries  # jobid, ievt, highest_part_pt, highest_track_pt\n",
    "  assert np.isfinite(x).all()\n",
    "\n",
    "  # Split dataset in training and testing\n",
    "  split = the_auxiliaries[:,0] < 50.\n",
    "  x_train, x_test, aux_train, aux_test = x[~split], x[split], aux[~split], aux[split]\n",
    "  return x_train, x_test, aux_train, aux_test\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "x_adv_train, x_adv_test, aux_adv_train, aux_adv_test = pileup_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create a model ####\n",
    "\n",
    "# See https://keras.io/models/about-keras-models/\n",
    "#     https://keras.io/layers/about-keras-layers/\n",
    "#     https://keras.io/getting-started/functional-api-guide/#getting-started-with-the-keras-functional-api\n",
    "\n",
    "def create_model():\n",
    "  inputs = Input(shape=(nvariables,), dtype='float32')\n",
    "\n",
    "  x = Dense(64, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(inputs)\n",
    "  #x = Dropout(0.2)(x)\n",
    "  x = Dense(32, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(x)\n",
    "  #x = Dropout(0.2)(x)\n",
    "  x = Dense(16, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000))(x)\n",
    "  #x = Dropout(0.2)(x)\n",
    "\n",
    "  regr = Dense(1, activation='linear', kernel_initializer='glorot_uniform', name='regr')(x)\n",
    "  discr = Dense(1, activation='sigmoid', kernel_initializer='glorot_uniform', name='discr')(x)\n",
    "\n",
    "  # This creates a model that includes\n",
    "  # the Input layer, three Dense layers and the Output layer\n",
    "  model = Model(inputs=inputs, outputs=[regr, discr])\n",
    "\n",
    "  # Set loss and optimizers\n",
    "  #binary_crossentropy = losses.binary_crossentropy\n",
    "  #mean_squared_error = losses.mean_squared_error\n",
    "\n",
    "  adam = optimizers.Adam(lr=0.00113)\n",
    "  #adam = optimizers.Adam(lr=0.001)  # default\n",
    "  #adam = optimizers.Adam(lr=0.01)\n",
    "  #adam = optimizers.Adam(lr=0.001, amsgrad=True)\n",
    "\n",
    "  # Compile\n",
    "  model.compile(optimizer=adam,\n",
    "    loss={'regr': masked_huber_loss, 'discr': masked_binary_crossentropy},\n",
    "    loss_weights={'regr': 1.0, 'discr': discr_loss_weight},\n",
    "    #metrics={'regr': ['acc', 'mse', 'mae'], 'discr': ['acc',]}\n",
    "    )\n",
    "  return model\n",
    "\n",
    "def create_model_sequential():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(64, input_dim=nvariables, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000)))\n",
    "  #model.add(Dropout(0.2))\n",
    "  model.add(Dense(32, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000)))\n",
    "  #model.add(Dropout(0.2))\n",
    "  model.add(Dense(16, activation='tanh', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2(0.0000)))\n",
    "  #model.add(Dropout(0.2))\n",
    "  model.add(Dense(1, activation='linear', kernel_initializer='glorot_uniform'))\n",
    "  \n",
    "  adam = optimizers.Adam(lr=0.001)\n",
    "  model.compile(loss=huber_loss, optimizer=adam, metrics=['acc'])\n",
    "  return model\n",
    "\n",
    "def save_model(model):\n",
    "  # Store model to file\n",
    "  model.summary()\n",
    "  model.save('model.h5')\n",
    "  model.save_weights('model_weights.h5')\n",
    "\n",
    "  # Store model to json\n",
    "  import json\n",
    "  with open('model.json', 'w') as outfile:\n",
    "    outfile.write(model.to_json())\n",
    "  return\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Training Functions ####\n",
    "\n",
    "# from https://github.com/keras-team/keras/blob/master/keras/utils/generic_utils.py\n",
    "def slice_arrays(arrays, start=None, stop=None):\n",
    "    \"\"\"Slices an array or list of arrays.\n",
    "    This takes an array-like, or a list of\n",
    "    array-likes, and outputs:\n",
    "        - arrays[start:stop] if `arrays` is an array-like\n",
    "        - [x[start:stop] for x in arrays] if `arrays` is a list\n",
    "    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n",
    "    # Arguments\n",
    "        arrays: Single array or list of arrays.\n",
    "        start: can be an integer index (start index)\n",
    "            or a list/array of indices\n",
    "        stop: integer (stop index); should be None if\n",
    "            `start` was a list.\n",
    "    # Returns\n",
    "        A slice of the array(s).\n",
    "    \"\"\"\n",
    "    if arrays is None:\n",
    "        return [None]\n",
    "    elif isinstance(arrays, list):\n",
    "        if hasattr(start, '__len__'):\n",
    "            # hdf5 datasets only support list objects as indices\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return [None if x is None else x[start] for x in arrays]\n",
    "        else:\n",
    "            return [None if x is None else x[start:stop] for x in arrays]\n",
    "    else:\n",
    "        if hasattr(start, '__len__'):\n",
    "            if hasattr(start, 'shape'):\n",
    "                start = start.tolist()\n",
    "            return arrays[start]\n",
    "        elif hasattr(start, '__getitem__'):\n",
    "            return arrays[start:stop]\n",
    "        else:\n",
    "            return [None]\n",
    "\n",
    "\n",
    "def merge_arrays(arrays, arrays_to_add):\n",
    "    if isinstance(arrays, list):\n",
    "        return [None if x is None else np.concatenate((x,y)) for (x,y) in zip(arrays, arrays_to_add)]\n",
    "    else:\n",
    "        return [None]\n",
    "\n",
    "# from https://github.com/keras-team/keras/blob/master/keras/engine/training_utils.py\n",
    "def make_batches(size, batch_size):\n",
    "    \"\"\"Returns a list of batch indices (tuples of indices).\n",
    "    # Arguments\n",
    "        size: Integer, total size of the data to slice into batches.\n",
    "        batch_size: Integer, batch size.\n",
    "    # Returns\n",
    "        A list of tuples of array indices.\n",
    "    \"\"\"\n",
    "    num_batches = (size + batch_size - 1) // batch_size  # round up\n",
    "    return [(i * batch_size, min(size, (i + 1) * batch_size))\n",
    "            for i in range(num_batches)]\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "# from https://github.com/keras-team/keras/blob/2.0.5/keras/engine/training.py\n",
    "\n",
    "import copy\n",
    "from keras import callbacks as cbks\n",
    "from keras.utils.generic_utils import Progbar\n",
    "\n",
    "def train(model, x, y, x_adv, aux_adv, batch_size=None, epochs=1, verbose=1, callbacks=None,\n",
    "          validation_split=0., shuffle=True, class_weight=None, sample_weight=None):\n",
    "  \n",
    "  # Validate user data.\n",
    "  x, y, sample_weights = model._standardize_user_data(\n",
    "    x, y,\n",
    "    sample_weight=sample_weight,\n",
    "    class_weight=class_weight,\n",
    "    batch_size=batch_size)\n",
    "  \n",
    "  y = [y[0] * reg_pt_scale, y[1]]\n",
    "  \n",
    "  # Prepare input arrays\n",
    "  if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "    ins = x + y + sample_weights + [1.]\n",
    "  else:\n",
    "    ins = x + y + sample_weights\n",
    "\n",
    "  #print('[INFO] ins shapes: {0}'.format([xx.shape for xx in ins]))\n",
    "  \n",
    "  # Prepare validation data.\n",
    "  do_validation = False\n",
    "  if validation_split and 0. < validation_split < 1.:\n",
    "    do_validation = True\n",
    "    if hasattr(x[0], 'shape'):\n",
    "      split_at = int(x[0].shape[0] * (1. - validation_split))\n",
    "    else:\n",
    "      split_at = int(len(x[0]) * (1. - validation_split))\n",
    "    x, val_x = (slice_arrays(x, 0, split_at), slice_arrays(x, split_at))\n",
    "    y, val_y = (slice_arrays(y, 0, split_at), slice_arrays(y, split_at))\n",
    "    sample_weights, val_sample_weights = (slice_arrays(sample_weights, 0, split_at), slice_arrays(sample_weights, split_at))\n",
    "    if model.uses_learning_phase and not isinstance(K.learning_phase(), int):\n",
    "      val_ins = val_x + val_y + val_sample_weights + [0.]\n",
    "    else:\n",
    "      val_ins = val_x + val_y + val_sample_weights\n",
    "  else:\n",
    "    val_ins = []\n",
    "\n",
    "  # logic from `_fit_loop()`\n",
    "  num_train_samples = x[0].shape[0]\n",
    "  index_array = np.arange(num_train_samples)\n",
    "  num_test_samples = val_x[0].shape[0]\n",
    "  val_index_array = np.arange(num_test_samples)\n",
    "  \n",
    "  # Callbacks\n",
    "  out_labels = model.metrics_names\n",
    "  if do_validation:\n",
    "    callback_metrics = copy.copy(out_labels) + ['val_' + n for n in out_labels]\n",
    "  else:\n",
    "    callback_metrics = copy.copy(out_labels)\n",
    "\n",
    "  model.history = cbks.History()\n",
    "  callbacks = [cbks.BaseLogger()] + (callbacks or []) + [model.history]\n",
    "  if verbose:\n",
    "    callbacks += [cbks.ProgbarLogger()]\n",
    "  callbacks = cbks.CallbackList(callbacks)\n",
    "  callback_model = model\n",
    "  callbacks.set_model(callback_model)\n",
    "  callbacks.set_params({\n",
    "      'batch_size': batch_size,\n",
    "      'epochs': epochs,\n",
    "      'samples': num_train_samples,\n",
    "      'verbose': verbose,\n",
    "      'do_validation': do_validation,\n",
    "      'metrics': callback_metrics or [],\n",
    "  })\n",
    "  callbacks.on_train_begin()\n",
    "  callback_model.stop_training = False\n",
    "  for cbk in callbacks:\n",
    "      cbk.validation_data = val_ins\n",
    "  \n",
    "  \n",
    "  # Loop over epochs\n",
    "  for epoch in xrange(epochs):\n",
    "    epoch_logs = {}\n",
    "    callbacks.on_epoch_begin(epoch)\n",
    "    \n",
    "    if shuffle:\n",
    "      np.random.shuffle(index_array)\n",
    "      #np.random.shuffle(val_index_array)\n",
    "    \n",
    "    batches = make_batches(num_train_samples, batch_size)\n",
    "    \n",
    "    # Loop over batches\n",
    "    for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "      batch_ids = index_array[batch_start:batch_end]\n",
    "      if ins and isinstance(ins[-1], float):\n",
    "        # Do not slice the training phase flag.\n",
    "        ins_batch = slice_arrays(ins[:-1], batch_ids) + [ins[-1]]\n",
    "        assert isinstance(ins_batch, list) and len(ins_batch) == 1 + 2 + 2 + 1\n",
    "      else:\n",
    "        ins_batch = slice_arrays(ins, batch_ids)\n",
    "        assert isinstance(ins_batch, list) and len(ins_batch) == 1 + 2 + 2\n",
    "      \n",
    "      # Add noise (pileup)\n",
    "      if add_noise:\n",
    "        #noise = x_adv[np.random.randint(0, x_adv.shape[0], ins_batch[0].shape[0])]\n",
    "        #noise_reg = np.zeros_like(ins_batch[1]) + 100.  # mask_value is set to 100\n",
    "        #noise_discr = np.zeros_like(ins_batch[2])\n",
    "        #noise_reg_w = np.ones_like(ins_batch[3])\n",
    "        #noise_discr_w = np.ones_like(ins_batch[3])\n",
    "        n = np.sum(np.equal(ins_batch[2],1.))\n",
    "        noise = x_adv[np.random.randint(0, x_adv.shape[0], n)]\n",
    "        noise_reg = np.zeros((n,1)) + 100.  # mask_value is set to 100\n",
    "        noise_discr = np.zeros((n,1))\n",
    "        noise_reg_w = np.ones((n,))\n",
    "        noise_discr_w = np.ones((n,))\n",
    "        ins_noise = [noise, noise_reg, noise_discr, noise_reg_w, noise_discr_w]\n",
    "        if ins and isinstance(ins[-1], float):\n",
    "          ins_batch = merge_arrays(ins_batch[:-1], ins_noise) + [ins_batch[-1]]\n",
    "        else:\n",
    "          ins_batch = merge_arrays(ins_batch, ins_noise)\n",
    "      \n",
    "      batch_logs = {}\n",
    "      batch_logs['batch'] = batch_index\n",
    "      batch_logs['size'] = len(batch_ids)\n",
    "      callbacks.on_batch_begin(batch_index, batch_logs)\n",
    "      \n",
    "      # Magic\n",
    "      model._make_train_function()\n",
    "      f = model.train_function\n",
    "      outs = f(ins_batch)\n",
    "      \n",
    "      if not isinstance(outs, list):\n",
    "        outs = [outs]\n",
    "      for l, o in zip(out_labels, outs):\n",
    "        batch_logs[l] = o\n",
    "      \n",
    "      callbacks.on_batch_end(batch_index, batch_logs)\n",
    "      if callback_model.stop_training:\n",
    "        break\n",
    "      \n",
    "      if batch_index == len(batches) - 1:  # Last batch.\n",
    "        if do_validation:\n",
    "          # logic from `_test_loop()`\n",
    "          val_batches = make_batches(num_test_samples, batch_size)\n",
    "          val_outs = []\n",
    "          if verbose == 1:\n",
    "            progbar = Progbar(target=num_test_samples)\n",
    "          \n",
    "          for val_batch_index, (val_batch_start, val_batch_end) in enumerate(val_batches):\n",
    "            val_batch_ids = val_index_array[val_batch_start:val_batch_end]\n",
    "            if isinstance(val_ins[-1], float):\n",
    "              # Do not slice the training phase flag.\n",
    "              val_ins_batch = slice_arrays(val_ins[:-1], val_batch_ids) + [val_ins[-1]]\n",
    "            else:\n",
    "              val_ins_batch = slice_arrays(val_ins, val_batch_ids)\n",
    "            \n",
    "            # Magic\n",
    "            model._make_test_function()\n",
    "            val_f = model.test_function\n",
    "            val_batch_outs = val_f(val_ins_batch)\n",
    "            \n",
    "            if isinstance(val_batch_outs, list):\n",
    "              if val_batch_index == 0:\n",
    "                for i, val_batch_out in enumerate(val_batch_outs):\n",
    "                  val_outs.append(0.)\n",
    "              for i, val_batch_out in enumerate(val_batch_outs):\n",
    "                val_outs[i] += val_batch_out * len(val_batch_ids)\n",
    "            else:\n",
    "              if val_batch_index == 0:\n",
    "                val_outs.append(0.)\n",
    "              val_outs[0] += val_batch_outs * len(val_batch_ids)\n",
    "            \n",
    "            if verbose == 1:\n",
    "              progbar.update(val_batch_end)\n",
    "            \n",
    "          if not isinstance(val_outs, list):\n",
    "            val_outs = [val_outs]\n",
    "          # Same labels assumed.\n",
    "          for l, o in zip(out_labels, val_outs):\n",
    "            o /= num_test_samples\n",
    "            epoch_logs['val_' + l] = o\n",
    "            \n",
    "    callbacks.on_epoch_end(epoch, epoch_logs)\n",
    "    if callback_model.stop_training:\n",
    "      break\n",
    "\n",
    "  callbacks.on_train_end()\n",
    "  return model.history\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "def predict(model, x):\n",
    "  outs = model.predict(x)\n",
    "  outs[0] /= reg_pt_scale\n",
    "  return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Begin training ...\n",
      "[INFO] Time elapsed: 19999.800952 sec\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 68)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 64)            4416        input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            2080        dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 16)            528         dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "regr (Dense)                     (None, 1)             17          dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "discr (Dense)                    (None, 1)             17          dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 7,058\n",
      "Trainable params: 7,058\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "[INFO] Done training.\n",
      "[INFO] Model is saved as model.h5, model.json and model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "#### Training ####\n",
    "\n",
    "print('[INFO] Begin training ...')\n",
    "assert keras.backend.backend() == 'tensorflow'\n",
    "\n",
    "start_time = time.time()\n",
    "sys.stdout = open('keras_output_1.txt', 'w')\n",
    "#history = model.fit(x_train, y_train, epochs=20, validation_split=0.1, batch_size=256, verbose=1)\n",
    "#history = model.fit(x_train, y_train, epochs=200, validation_split=0.1, batch_size=256, callbacks=[lr_decay], verbose=0)\n",
    "#history = train(model, x_train, y_train, x_adv_train, aux_adv_train, epochs=10, validation_split=0.1, batch_size=128, verbose=1)\n",
    "history = train(model, x_train, y_train, x_adv_train, aux_adv_train, epochs=300, validation_split=0.1, batch_size=128, callbacks=[lr_decay], verbose=1)\n",
    "sys.stdout.close()\n",
    "sys.stdout = old_stdout\n",
    "print('[INFO] Time elapsed: {0} sec'.format(time.time() - start_time))\n",
    "\n",
    "save_model(model)\n",
    "print('[INFO] Done training.')\n",
    "print('[INFO] Model is saved as model.h5, model.json and model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1085000/1093144 [============================>.] - ETA: 0s[INFO] loss and metrics: [30.111731605945472, 30.08605770553768, 0.025673895914748043]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUkiIiEhACgkREQlIISEiIgH9P7PbtdehY50IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa0ac8eba50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEZCAYAAABiu9n+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd8VFX6x/HPk0oCoYZeQu8goKKALMGCggULIAL6Ezv2\nsougq6Is4uq6NhREBRHFAugqIoKCEUG6hA5SQktogQTS6/P7Y24ghAwEk5kw4Xm/XvPi5t4zZ87J\nDfnmnNtEVTHGGGMK41faDTDGGHPuspAwxhjjloWEMcYYtywkjDHGuGUhYYwxxi0LCWOMMW5ZSBhT\nwkQkV0Qal3Y7jCkJFhLGlDy7+MiUGRYS5rwhIv5eqk9K8nOMKU0WEqZME5EYERkuImuAZBHxE5Ha\nIjJDRA6KyHYReSRf+XIiMkVEjojIBhH5h4jsOV19Z/j8iiLyifNZMSLybL5tTUQkSkQSne2f59v2\nhogcEJGjIrJGRFp75jtkzOkFlHYDjPGCgUBv4LAzFTQL+Aa4FagP/Cwim1X1J2AU0ABoCFQA5hQy\nfXS8PlXNPcNnjwPCnPqqA/NEJE5VJwOjgbmqGikiQcBFuAKiF3AZ0FRVk0SkBZDouW+PMe7ZSMKc\nD95S1ThVzQAuBsJVdYyq5qjqTuBD5xc/QH9gjKoeU9U44O0z1OeWM8q4FRihqqmqugt4HbjdKZIF\nRIhIXVXNVNXf860PA1qLiKjqFlU9ULLfEmOKxkLCnA/25luOAOo600lHRCQBGAnUcLbXKVB+D6fa\nW8i6woQ7o/Xd+dbtAuo6y8Od/4PLRWSdiAwFUNVfnBHIu8ABEZkgIhXOrsvGlAwLCXM+yD9dtAfY\noapVnVcVVa2kqtc72+OAevnKNzhDfacTnzdayLcuAojFFQYHVPU+Va0LPAC8l3fqrKqOU9WLgNZA\nC+AfZ91rY0qAhYQ53ywHkpyDz+VExF9E2ojIRc726cBIEaksInWBh/7qBznHK74CxohIBRGJAJ4A\npuKajurnfAbOMYdcIFdELhKRziISAKQB6c42Y7zOQsKUdSf91e/84r4O6ADEAAeBD4CKTpGXnL/0\nY4B5TmhkuKuvCJ/5KJAK7AAWAp86B61xjo8sE5FjwP+AR51jJBWdNh1x2hEPvFb8b4UxZ0+88dAh\n5wDeSmCvqt5QyPa3nbNFUoA7VTXa440ypghE5AHgVlXtWdptMaY0eGsk8RiwsbANItIbaKKqzYD7\ngQleapMxpxCRWiLSVVxaAE8BX5d2u4wpLR4PCRGpB/RxTjMsTF/gE1xTAcuASiJS09PtMsaNIOB9\n4Bjws3M9xfjSbpQxpcUbF9O94ZyZUcnN9roFTjOMddbZeeHG61R1N9CutNthzLnCoyMJEbkWOOAc\nYxC7p40xxvgWT48kugE3iEgfIAQIE5FPVPWOfGVinVsj5KmXdx55fiJid9Y0xpi/QFX/8h/oHh1J\nqOozqtpAVRs7tz1YUCAgAL4D7sAVBJcCie5uQaCqZfb1wgsvlHobrH/Wv/Otb+dD/4qrVG7wJyL3\nu37n60RV/UFE+ojINucU2KGl0SZjjDGn8lpIqOqvwK/O8vsFtj3srXYYY4wpOrvi+hwRGRlZ2k3w\nKOuf7yrLfeM86F9xeeWK65LgumOyb7TVGGPOFSJCcQ5c+/xDhxo2bMiuXbtKuxmmEBEREezcubO0\nm2GMKQafH0k4KVkqbTKnZ/vGmNJX3JGEHZMwxhjjloWEMcYYtywkjDHGuGUhcY4bNmwYY8aMKe1m\nGGPOU3bg2sMaNWrERx99xOWXX17aTfG6c33fGHM+sAPXPiwnJ6e0m2CMMadlIeFBd9xxB7t37+a6\n666jYsWKvPbaa/j5+TFp0iQiIiK44oorABgwYAC1a9emSpUqREZGsnHjiYf4DR06lOeffx6AX3/9\nlfr16/Pf//6XmjVrUrduXT7++ONS658xpuyzkPCgTz75hAYNGjB79myOHTvGgAEDAFi4cCGbN29m\n7ty5APTp04ft27dz8OBBOnXqxODBg93WuX//fpKSkoiLi+PDDz/koYce4ujRo17rkzHm/HJehIRI\n8V/FkX9eXkR48cUXCQkJITg4GIA777yT0NBQAgMDef7551mzZg1JSUmF1hUUFMRzzz2Hv78/vXv3\npkKFCmzZsqV4DTTGGDfOi5BQLf6rJNWrV+/4cm5uLiNGjKBp06ZUrlyZRo0aISLEx8cX+t5q1arh\n53dit4WGhpKcnFyyDTTGGMd5ERKlSQoZhuRfN23aNGbNmsWCBQtITExk586dJfawEGOMKS4LCQ+r\nVasWO3bsgHxP1ssvKSmJ4OBgqlSpQkpKCiNHjiw0WIwxpjR4NCREJFhElonIahHZICIvF1KmmojM\nEZFoEVknInd6sk3eNmLECEaPHk3VqlWZOXPmKQFwxx130KBBA+rWrUvbtm3p2rXrWdVvgWKM8SSP\nX0wnIqGqmioi/sBi4ClVXZxv+wtAOVUdKSLhwBagpqpmF6jHJy+mO5/ZvjGm9J3zF9OpaqqzGOx8\nXkKBIvuBMGc5DDhcMCCMMcaUDo+HhIj4ichqJwyiVHVjgSIfAG1EJA5YAzzm6TYZY4wpGo8/mU5V\nc4GOIlIRmCciPVT113xFRgJrVLWniDQBfhKR9qp6ynmdo0aNOr4cGRlpz6Y1xpgCoqKiiIqKKrH6\nvHqDPxF5DkhV1dfzrfsBGJN3nEJE5gNPq+rKAu+1YxI+xvaNMaXvnD4mISLhIlLJWQ4BrgKiCxTb\nBFzplKkJNAd2eLJdxhhjisbT0021gSniOk/TD5iqqvNF5H7XTJROBMYCk0VkDSDAcFU94uF2GWOM\nKQJ7noTxGNs3xpS+c3q6yRhjjG+zkDgH5T03Ik/btm1ZuHBhkcqeLXs8qjHmdDx+Cqz5a/LfbmP9\n+vVFLns6U6ZM4cMPP+S33347vm78+PHFaKUxpqyzkcR5RFXtXk/GmLNiIeFBr776Kv379z9p3eOP\nP87jjz/Oxx9/TOvWralYsSJNmzZl4sSJbutp1KgRCxYsACA9PZ0777yTqlWr0rZtW1asWHFS2X//\n+980bdqUihUr0rZtW/73v/8BsHnzZoYNG8aSJUsICwujatWqUODxqAAffPABzZo1Izw8nBtvvJF9\n+/Yd3+bn58f7779P8+bNqVq1Kg8//HAJfaeMMecqCwkPGjhwIHPmzCElJQWcBwx99dVXDBo0iJo1\nax5/rOnkyZN54okniI4ueAnJqUaNGkVMTAwxMTHMnTuXKVOmnLS9adOmLF68mGPHjvHCCy8wZMgQ\nDhw4QMuWLZkwYQJdunQhKSmJI0dOPct4wYIFPPPMM8yYMYN9+/bRoEEDBg4ceFKZ2bNns2rVKtas\nWcNXX33FvHnziv19Msacu86LYxLyYvGnWPSFsz+Vs0GDBnTq1IlvvvmGIUOGMH/+fMqXL0/nzp1P\nKte9e3d69erFb7/9RocOHU5b5/Tp05kwYQKVKlWiUqVKPProo4wePfr49ltuueX4cv/+/Xn55ZdZ\nvnw5119//RnbO23aNO6++24uuOACAMaOHUuVKlXYvXs3DRo0AGDkyJGEhYURFhZGz549iY6Oplev\nXmf9vTHG+IbzIiT+yi/4knLbbbfx+eefM2TIED7//HMGDRoEwJw5c3jppZf4888/yc3NJS0tjfbt\n25+xvri4uJMefxoREXHS9k8++YQ33niDnTt3ApCSkuL2UaiF1X3hhRce/7p8+fJUq1aN2NjY4yFR\ns2bN49vt0anGlH023eRh/fv3JyoqitjYWL755hsGDx5MZmYm/fr1Y/jw4Rw6dIiEhAR69+5dpAvP\nateuzZ49e45/vWvXruPLu3fv5r777uO9994jISGBhIQE2rRpc7zeMx20rlOnzkn1paSkcPjw4ZNC\nyRhzfrGQ8LDw8HB69OjB0KFDady4Mc2bNyczM5PMzEzCw8Px8/Njzpw5RZ7bHzBgAGPHjiUxMZG9\ne/cybty449tSUlLw8/MjPDyc3NxcJk+efNLpszVr1mTv3r1kZWUVWvdtt93G5MmTWbt2LRkZGTzz\nzDNceumlxboOwxjj2ywkvGDQoEHMnz+fwYMHA1ChQgXefvtt+vfvT9WqVfniiy/o27ev2/fnHwG8\n8MILNGjQgEaNGnHNNddwxx13HN/WqlUrnnrqKS699FJq1arFhg0buOyyy45vv/zyy2nTpg21atWi\nRo0ap3zOFVdcwejRo7n55pupW7cuMTExfPHFF4W2o7CvjTFlj927yXiM7RtjSp/du8kYY4zHWEgY\nY4xxy0LCGGOMWxYSxhhj3PL040uDRWSZiKwWkQ0i8rKbcpFOmfUi8osn22SMMaboPH52k4iEqmqq\niPgDi4GnVHVxvu2VgN+BXqoaKyLhqnrKJcJ2dpPvsX1jTOkr7tlNHr8th6qmOovBzsgloUCRQcBM\nVY11yhftHhKOiIgIO1//HFXwliHGGN/j8ZAQET9gFdAEmKCqGwsUaQ4EOtNMFYC3VXVqYXWpQsE8\nyLtHkTHGmJLnjZFELtBRRCoC80Skh6r+WqANnYDLgfLAEhFZoqrbCtY1atSo4yERGRlJZGSkp5tv\njDE+JSoqiqioqBKrz6tXXIvIc0Cqqr6eb93TQDlVfdH5+kNgjqrOLPBezcpSAs6L+9YaY0zJOKev\nuBaRcOfANCISAlwFFHyyzrfAZSLiLyKhwCXApsLqs2OgxhjjXZ7+u7w2MEVcR5b9gKmqOl9E7nfN\nROlEVd0sInOBtUAOMLGQ4xYA5OZ6uLXGGGNO4lM3+EtNVUJCSrslxhjjO87p6aaSZiMJY4zxLp8K\nCR8Z9BhjTJnhUyFhIwljjPEuCwljjDFu+VRI2HSTMcZ4l0+FhI0kjDHGuywkjDHGuOVTIWHTTcYY\n410+FRI2kjDGGO+ykDDGGOOWhYQxxhi3fCok7JiEMcZ4l0+FhI0kjDHGuywkjDHGuOVTIWHTTcYY\n410+FRI2kjDGGO/y9ONLg0VkmYisFpENIvLyacpeLCJZInKzuzIWEsYY410efXypqmaISE9VTRUR\nf2CxiHRT1cX5y4mIH/AKMPf09XmytcYYYwry+HSTqqY6i8HO5yUUUuwRYAZw8HR12UjCGGO8y+Mh\nISJ+IrIa2A9EqerGAtvrADeq6njgtM9htZAwxhjv8uh0E66RRC7QUUQqAvNEpIeq/pqvyJvA0/m+\ndhsU48aNomZN13JkZCSRkZEebLkxxvieqKgooqKiSqw+US9O9IvIc0Cqqr6eb92OvEUgHEgB7lPV\n7wq8V9esUdq391pzjTHG54kIqnraWZrT8ehIQkTCgSxVPSoiIcBVwIv5y6hq43zlJwOzCgZEHptu\nMsYY7/L0dFNtYIqIiHP8Y6qqzheR+135oBMLlD/tsMZCwhhjvMur003FISK6cqVy4YWl3RJjjPEd\nxZ1usiuujTHGuGUhYYwxxi2fCgkfmRkzxpgyw6dCwkYSxhjjXRYSxhhj3LKQMMYY45ZPhYQdkzDG\nGO/yqZCwkYQxxniXhYQxxhi3fCokbLrJGGO8y6dCwkYSxhjjXRYSxhhj3PKpkLDpJmOM8S6fCgkb\nSRhjjHdZSBhjjHHLoyEhIsEiskxEVovIBhF5uZAyg0RkjfNaJCLt3NVnIWGMMd7l0SfTqWqGiPRU\n1VQR8QcWi0g3VV2cr9gO4G/OI06vAT4ALi28Pk+21hhjTEGefnwpqprqLAY7I5eEAtuX5vtyKVDX\nXV02kjDGGO/y+DEJEfETkdXAfiBKVTeepvg9wBx3Gy0kjDHGuzweEqqaq6odgXrA30SkR2HlRKQn\nMBR42n1dHm2qMcaYAjw+3ZRHVY+JyGzgIuDX/NtEpD0wEbhGVRPc1fHll6PYsMG1HBkZSWRkpOcb\nbowxPiQqKoqoqKgSq0/Ug3+ei0g4kOUclA4B5gIvqur8fGUaAPOB2wscnyhYl06bptx2m8eaa4wx\nZY6IoKryV9/v6ZFEbWCKiIgztTVVVeeLyP2uwYVOBJ4DqgLvOeWyVLVzYZXZdJMxxniXR0cSJUlE\ndOpUZciQ0m6JMcb4juKOJOyKa2OMMW4VKSRE5DERqSguH4nIHyLSy/PNO5mFhDHGeFdRRxJ3qeox\noBdQBbgdeMXDbTuFj8yMGWNMmVHUkMibz+rjHHzekG+d19hIwhhjvKuoIbFKROY5ITFXRMIAr//K\ntpAwxhjvKuopsHcDHYAdzs36qjpXR3uVTTcZY4x3FXUk0QXYoqqJIjIE+Cdw1MNtO4WNJIwxxruK\nGhLjgVQRuQB4CtgOfOLhtp3CQsIYY7yrqCGRra6r7voC41T1XSDMw207hU03GWOMdxX1mESSiIx0\nTn3tLiJ+QKCH23YKG0kYY4x3FXUkcSuQ4Vwvsd+57fdrHm7bKSwkjDHGu4oUEk4wfAZUEpHrgHRV\ntWMSxhhTxhX1thwDgOVAf2AAsExE+nm+eSezYxLGGONdRT0m8SxwsaoexBUa1YGfgRmebd7JbCRh\njDHeVdRjEn55AeE4XBp3kLWQMMYY7yrqSOJHEZkLfO58fSvwgwfbVSibbjLGGO8qUkio6j9E5Bag\nm7Nqoqp+c6b3iUgwsBAIcl7fquozhZR7G+gNpAB3qmp0YfXZSMIYY7yryI8vVdWZwMyzqVxVM0Sk\np3O/J39gsYh0U9XFeWVEpDfQRFWbicglwATg0sLqs5AwxhjvOm1IiEgSUNgkjzjPqK54pg9Q1VRn\nMdg5jpFQoEjfvFt8qOoyEakkIjVV9cCpdZ3p04wxxpSk04aEqhb71hvO1dmrgCbABFXdWKBIXWBP\nvq9jnXWnhISNJIwxxrs8foaSquaqakfnKu2/iUiPv1qXhYQxxnhXkY9JFJeqHhOR2cBFwK/5NsUC\n9fN9Xc9Zd4pffhl1fDkyMpLIyEhPNtkYY3xOVFQUUVFRJVafqAcn+kUkHMhS1aMiEgLMBV5U1fn5\nyvQBHlLVa0XkUuBNVT3lwLWI6D//qYwe7bHmGmNMmSMiqOpffty0p0cStYEpIiLO1NZUVZ0vIvc7\nB74nquoPItJHRLY5p8C6feKdTTcZY4x3eTQkVHUd0KmQ9e8X+PrhotRnIWGMMd7l9VtrFIedAmuM\nMd7lUyFhIwljjPEuCwljjDFu+VRI2HSTMcZ4l0+FhI0kjDHGuywkjDHGuGUhYYwxxi2fCgk7JmGM\nMd7lUyFhIwljjPEuCwljjDFu+VRI2HSTMcZ4l0+FhI0kjDHGuywkjDHGuOVTIWHTTcYY410+FRI2\nkjDGGO+ykDDGGOOWR0NCROqJyAIR2SAi60Tk0ULKVBOROSIS7ZS50119FhLGGONdnn58aTbwpKpG\ni0gFYJWIzFPVzfnKPAxEq2pv55nYW0TkU1XNLliZHZMwxhjv8uhIQlX3q2q0s5wMbALqFii2Hwhz\nlsOAw4UFBDaSMMYYr/P0SOI4EWkIdACWFdj0ATBfROKACsCt7uqwkDDGGO/ySkg4U00zgMecEUV+\nI4E1qtpTRJoAP4lI+0LKsX79KEaNci1HRkYSGRnpjeYbY4zPiIqKIioqqsTqE/XwRL+IBADfA3NU\n9a1Ctv8AjFHVxc7X84GnVXVlgXJ6yy3KjBkeba4xxpQpIoKqyl99vzdOgZ0EbCwsIBybgCtxdaYm\n0BzYUVhBm24yxhjv8uh0k4h0AwYD60RkNaDAM0CE61i2TgTGApNFZA0gwHBVPVJYfXZ2kzHGeJdH\nQ8KZQvI/Q5l44Pqi1GcjCWOM8S674toYY4xbFhLGGGPc8qmQsGMSxhjjXT4VEjaSMMYY77KQMMYY\n45ZPhYRNNxljjHf5VEjYSMIYY7zLQsIYY4xbPhUSNt1kjDHe5VMhYSMJY4zxLgsJY4wxbllIGGOM\nccunQsKOSRhjjHf5VEjYSMIYY7zLQsIYY4xbPhUSNt1kjDHe5dGQEJF6IrJARDaIyDoRedRNuUgR\nWS0i60XkF3f12UjCGGO8y6NPpgOygSdVNVpEKgCrRGSeqm7OKyAilYB3gV6qGisi4e4qy82FgykH\nqVG+hoebbYwxBk+PJFR1v6pGO8vJwCagboFig4CZqhrLiceZFio+fT+t3m3lySYbY4zJx2vHJESk\nIdABWFZgU3Ogqoj8IiIrROR2d3Uc8FvFkbQjpGene77BxhhjPD7dBK6AqADMAB5zRhQF29AJuBwo\nDywRkSWquu2UxsrLZP8CI9NG0veavkRGRnqj+cYY4zOioqKIiooqsfpEPXzKkIgEAN8Dc1T1rUK2\nPw2UU9UXna8/dMrOLFBOKz/VhcSwJay6bxWdanfyaLuNMaYsEBFUVf7q+70x3TQJ2FhYQDi+BS4T\nEX8RCQUucY5dnOJYhRUAxKe6PWxhjDGmBHl0uklEugGDgXUishpQ4BkgwnWMWieq6mYRmQusBXKA\niaq6sbD6ciWb8jn1OJRyyJPNNsYY4/BoSKjqYsC/COX+A/ynKHWGJlzCoVQLCWOM8QafuuI6yC+Y\nzN3tbSRhjDFe4lMh0TK8BblJNdhxwELCGGO8wadCYvHdi7m4dXU27LSQMMYYb/CpkKgQVIErulRn\nd7yd3WSMMd7gUyEBcHX36iTlHCIurrRbYowxZZ/PhUREeHWCqhxizJjSbokxxpR9PhcSVcpVIcvv\nKJ9/k8i2U27cYYwxpiT5XEj4+/nz2CWPoQ+0Z+gTu+xBRMYY40E+FxIAr1/9Og93u4t1tf/O22+X\ndmuMMabs8vgN/kqKiGj+tqZlpdH87dakfz6Fxx4qx8XdE7i66dWoKjsSdtCkapNSba8xxpwLinuD\nP58NCYBJqycxYcmn/LE1ltBqh3nl6pdoXq05fT7rw/J7l9OhVodSa68xxpwLzuuQyMzJpNk7zagV\n2Ixt77xDzpCe3NzuWjbFbyQtK43V969G5Oy+N3kPNCoXUK5E22+MMaXBF24V7jFB/kF82e9Lvhz8\nIfM+a0XWvlZMXj2ZSddN5UDKAfYc21PkulSVzJxMek7pySuLXvFou40xxld45cl0nnRpvUsBaHgh\nTAp6iAempvKvJ5vSqffFrIhdQYNKDYpUT+/PerMybiVB/kGsP7jew632jMycTAL8AvATn85+Y8w5\npEz9Nrm1XT92/PM3cnNhycyL+OjHFWRmnvl9qsqy2GV8d9t3fH3r12w5vMUbzS1x93x3DzM2zijt\nZhhjypAyFRIAVSoG8fnnMHzIxSzds4KmTWHsWDh48ORyi3Yv4qYvbyIrJ4t9yfsI8Auga/2utKvR\njm1HtpGTm3NS+S/Wf0FWTlaR2zFryyxiEmJKqltFsjl+M9uPbPfqZxpjyjaPhoSI1BORBSKyQUTW\nicijpyl7sYhkicjNJfHZd19zETk1VzJtRjLbt0OLFtCrF4x6NZ5hXz/NzV/ezJ+H/2Tq2qlsOLiB\nNtXbAFA+qDzVQ6uz6+iu43VF74/mtpm3MX7leABWxK5gR8IO1uxfw6drPz3ls3Nycxg2exjfbP6m\nJLpSZDsTdxKbFOvVzzTGlG2eHklkA0+qahugC/CQiLQsWEhE/IBXgLkl9cHVy1enT7M+9J5fmxuH\nf09MDPQaupzXkjow+bNjNJq/jD6543kp6l9E748+HhIALcJbsCX+xJTT+yvfZ2Dbgfxr4b9ITE/k\nhagXeGvpW0yOnsyLv754ymdH7YwiNimWtQfWnrGdH6z6gMW7Fxe7vymZKRxKPURckt350BhTcjz9\n+NL9wH5nOVlENgF1gc0Fij4CzAAuLsnP//yWz1m8ezG3fHULE66bwBv7HuLTQePo0/gm5syBzz5r\nxN6KjXjuyGtcHzaKn4KgVStoXq0F41aMY0HMAh679DG+3PAl6x9czwPfP8APW39g6d6lbD2ylUC/\nQHYf3c3Ww1tpVq3Z8c+dunYqt7a5lXUH152xjRP/mMhFtS+iW4Nuxepr3sjHRhLGmJLktbObRKQh\n0AFYVmB9HeBGVe0pIp1L+nO7NejGv6/8N68ufpXhXYdzU6ubALjxRtdrzoYX6TOjO1mxbXh5Dmze\nDAkNO0L3H1kSEMf4JVO4oeZTJO+rw9VNrmbc8nGEBYeRlpVGQnYCg9oNYs62OcdD4mj6Ub7d8i0r\n711Ju/HtyM7NJsCv8G9zWlYa0fujScpIKnY/YxJiaBXeithjFhLGmJLjlZAQkQrOSOExVU0usPlN\n4On8xd3VM2rUqOPLkZGRREZGFunz/6/D//F/Hf6v0G2921zG+LTx3HHBxYQGutYdPjKULVuGsnRD\nHN/GTCV5zgh6joaIjr1YcvHDdK8ykKoVK+AfkswNzW/gn7/8k671u9Kpdic+W/cZVza+kiZVm1An\nrA7jV4znj/1/0LNhT25vfztZuVlM3zCdHg17sCtxF+1qtGPX0V3EHoulbsW6x9t1unApzM7EnXSp\n14Wpa6eSk5uDv59/oeVu/vJmHrr4Ia5ofAUAGw5u4MM/PuSNa94o8mcZY85dUVFRREVFlVh9Hr/i\nWkQCgO+BOar6ViHbd+QtAuFACnCfqn5XoNwpV1x7U1YWfPml8uCfjam/9wn8ttzEjl3ZhOVEkHnB\neDIvfJ2coMOICI/UnMFTN13J/b/cxJytc3j5ipf5OPpj6oTV4UjaEQC2HdlGl/pdaFS5EfuS93Fj\nixu5/YLbAVh/cD2XT7mcjQ9tpFpItZOuGk/PTic+NZ56Feud1L5/zPsH4aHh/Hfpf4m+P5raYbVP\n6UNSRhLVXq3Ggxc/yJvXvAnAf37/Dy9EvUDC0wkE+Qd5+LtojPE2X7jiehKwsbCAwHWsorHzauSM\nNh4sGBDngsBAGDJEmDbkHX5+qz/rFtcnYUcjVq30Y/m4h3ivxQ6GHNnOFft/YPvPV9CyJaz/9E5a\nbpjG8reepPO6JYRsHcJFWU/yWoulvH3JbH7e/jMdql/CHe3v4MVfXyQxPRGAv8/7O37ix5tL36TF\nuBbc8c0dJKQlAPD8L8/TYUIHNseffFhn59GdNKzckLphdY8fl0jOTOafC/7JiJ9HkJObw/yY+VQv\nX52fd/x8/H2/7/mdtKw0lu09aRbwL/lt12/83/8KH7EVlJqVyvQN04v9mUWhqmRkZ3jls4wpazw6\nkhCRbsCXmqTWAAAWN0lEQVRCYB2gzusZIML1f1cnFig/CfheVb8upK5SHUmcrUOHYPt2SEg48Tpw\nwLUuJgaOHYMjQdEc3tKSDm3LkXv1I+wM/JGqwTXI8UvhsWbv8mj0ZfRveieBwUpKdiITrptA63db\nM7zbcMavHM/iuxZTr2I9Nh7aSOTHkSy6axFPzn2SezvdS9+WfXln2TvM2DSDtKw07u10LyviVtCi\nWgvG/DaGHg170LRKUz5d9yl/i/gbrcJbMaT9EPzEj8ZVGv+lPj819yk+XP0hR4YfcTvdlee5Bc8x\ndtFY4ofHU7lc5b/4XS6aT9d+ythFY1l570pCAkM8+lnGnGuKO5Lw9NlNi4HT/7Y4ufxdnmyPN1Wv\n7nqdXgdycmDuXFi+4i3WH15O3NGDBO7sw4zcAOpWe4ffp9xJQnwQqYO68O36JtSMeZzUlBF0KO9P\nm7cupGpQbQ5mxfBos7dJ3N6ckKy6PP/zyyz8M5q5e2bwTp+3qRJShW6TuhEaGMrye5azNHYp+5L2\n8UvML4QEhnBXh7u46cubmLByAorSI6IH1zS9hu4NutOsWjO+3vQ1v+/5nTGXj+H9Ve9za5tbqVmh\nJrO2zKJz3c7UrFATgJ9jfiYjO4ONhzbSrma7QnuclpXGx9EfM37leNrXbM8vMb8cP5nAU+bHzOdQ\nyiFG/DyCt3oXOqA1xrjh03eBPZ8kJqdxJDmF7evC+e03OHoUdqVuIjE5HZJqo0m1SEuD+Aq/cixs\nKUl1ZpEdfIihKZupHi6khm6mSbVGhAQGk5KTSKCUY2HuKyQGbGHUBVNYenQmD/a8mVzN5c2lb7Ip\nfhM/bvuRLvW7sGTPEjJzMnn0kkeZunYqSRlJPNv9WUbMH8FlDS6jQlAF/MWfBTELuL7F9XSr3437\nL7yfzfGbaVi54Ul/vT+34DmidkUxuudoVsWtYtuRbYy/bnyRvgdH0o5QNaTqWX/vmr3TjEk3TGLQ\n14OYdMMkrmpyVZHetzPRNYXnbYt2L6JT7U6EBoZ6/bNN2XNe3yrcuJeWlcaarYdZMb8eycmQnAzx\n8ZCTA3nHwfcfUPbEZeCv5di3D2rUgDp1ICTE9Qoun87WKu9QOTicQ4ErWJoznufqLKZyeAbPbriW\n8X0+YOr6SURUimDrka2Eh4bTp2kfJkdPJj07nf3J+0nOTGbmgJn8vud3wkPDGb1wNFF3RtEyvCXr\nD67numnXsf7B9VQIqoCqsmrfKo6mH2XL4S3sT97Ps92fJTggmMT0RBq+2ZD3rn2PQe0G8eTcJ1l3\ncB0Trp1w0gOmVBURYfafs2lfsz2B/oG0frc18cPjWRCzgHu+u4ftj24/43RY9P5oOr3fiegHomlf\ns72nd9dx2bnZhL8azlNdnuK5Hs957XNN2WUhYUpETg4sWwaJiZCW5nqlpLheycmwJ3Ury/U92sW+\nwd69sDMumX27KlClqlK1ihBcIZWg8mkElD/K+ja30PbwczTJupFjVRYyr9oN1AytQ3puCk0rt2Rm\n358IDobgYOWheffy0/Z5PHrJo8zYOIP41HjqhNWhWmg1APYe28v0/tP5bO1nfLvlW/Yl72NEtxG8\nsvgVhl00jHdXvMs3t35D5XKVKR9Ynks/upTrml3HlDVTuKDWBdze/nZ+2PoD3w/6HoAOEzrw1jVv\nsfHQRm5udfPxqbL80rLS6DOtD3uP7aVfq36MvXIsi3cvJiE9gaubXE2gfyD7k/dz76x76d20N8Mu\nGoaI8OO2H2lerflJx3RyNfes7sq7bO8y+k3vR1pWGpse2kT18tU5lnGMvcf20rp66xLZ1+b8YiFh\nSk1ODuzff3KwpKVBevqJ5cREmLL8a1K2dSQzPYCMrFyy4yPIyHCVy8qCwMZL0M7vEBLbi24V7qBT\nRz/XSCZYWSJvMDv1OQThjVYrWXJ0Bn+mrOCW2n+nb4fuLDg2gdELR5OZk0nF4Ir0adqHHYk7GN51\nOGMXjWXrka28f937XNn4SgBeWfQKn6//nF2Ju6hRvgbPdH+GumF1iUuKIykziX1J+5iwagKRDSMZ\nedlI+k/vz7J7ltF+fHvqVaxHuYByzB40m3dXvMui3YuITYrl7o53M6T9EJq83YT6Feuz7J5lhASG\n8OO2H/n7vL8T/UB0ode8zNoyixVxKxjSfgjNqzUHYMzCMRxOO0xieiKtwlvxj27/4Nn5z/Lq76/y\nyhWv8FTXp8C5vqVV9VZnDKCzvd7GlD0WEsan5eZCRobrlZICixbB1q0ngiY9HZLS0klOyyQzqSK5\nuaDqeq1d63pfuXLgX2k/aa0+pPb2EZQLCiA4GAKDcggMFIKD/KhdG7p2hdTgGB7c0ph/t/2RTL9E\nFsd/R0LWAaoGV6dySCWCAwO5v92TXNqyEapKl4+6sDl+M/d0uodXr3qVR+c8yrLYZcSnxvNVv6+o\nFlqNLh+5rndpV6MdKVkpVAiqwPvXvU/nDzsTkxDDK1e+wuHUwzSp2oSOtToyeuFolscuJz07nb4t\n+vLpuk8Z0W0E9114H70/680z3Z+hXEA5/j7v7yy7ZxkRb0bwyU2fcPs3t/NVv6+Yt30eY34bw5jL\nxzCy+0g2HNyAv58/LcNdt0Vbd2Ad/ab34z9X/Yd7Z93L57d8Ts9GPcnVXICTgqXghZdJGUkE+gce\nfzLj2gNrGbtoLFNvmnrGsDmQfIB+0/vxVb+vCr1O53R2H93N9iPbiWwYedJ1QWsPrKVtjbb2jJRi\nsJAw563cXNdxlszME0FTcDkry/VvTAysWAGpqRDPZvwTWpKWBtnZJ15paSfeGxwMERFQLiSXjLAt\nVMxqRqB/AP4Byqa6IzhU/hf6xC2jfKjQtksci7PH0T/iISqHVGTokgupEVKHo1mHefKCfzFs4Y30\njriFLE1n1cHfGdh2IPdfeD9NqzalfFB5th/ZzuNzH2fO1jlcUu8S5g2ZR7mActR/oz79W/dn1b5V\nLLprEdPWTWPw14Pp06wPo3uO5tpp1/LYJY/x9rK3ycrN4vb2t1OlXBW+3/o99SrW4+tNX3N98+tJ\nSE/gp9t/ou8XfQG4otEVjF85nlbhrZgfM58BbQYwrvc4UrNSiZwSSfnA8lzZ+EqWxS5j6+GtBPkH\nMebyMQxsO5CPVn/Ewl0LKRdQjvDQcNYdXEdmTiZTb5rK0G+HEr0/mqEdhlI9tDpbDm9haIehRFSO\nYO+xvVxU5yK3+3LA9AEsiFnAlY2v5It+XwAwJXoKd357J98O/JYbWtxQ5J+L9Ox0Av0CCz3ulJ2b\njSBnPCZVHEkZSYQFhx3/Oic3Bz/xO+tHKZcUCwljSpgq7NoFe/e6AiN/kGRnu6bZMrNyyc3xIyEB\nFi50TavlBVOSfwxHKy2hXOxVkBpOWpWVBBy4mLQ0V/3Nm0PFiuDnB/7+EBDgulhTAjMoFxBMQAAE\nBUFcw/9wIGgxA+v8k1aVLyQwUDmcG0PjKo0JDoa4jD8ZvnQIfZsO4PJGlxO190cSM+NJzEjgoxs+\nIiYhhojKEbQf356DKQfp2agnWTlZrD2wlkl9J7EzcSdXNb6Kl3596fjo6MGLHyQkIIStR7ZyQ4sb\nCAkIITMnk7u+u4tqIdWoVK4SQzsMJSM7gyNpR2hQqQGL9yzmgz8+oF/rfjz/t+fp8H4HWlRrwe3t\nb+e1318jOCCY1KxUbm1zK82qNqNz3c78uO1HalWoRWxSLC3DW/LE3Cf48+E/6fJRF+678D5m/TmL\nXYm76N+6PzsSd/BO73dYFbeKqJ1RbE/YztVNrmbYxcNIykji5x0/065mOxpXaczzvzzP28veplX1\nVvw4+EeOZRyjXsV6+Pv5E5cUR/fJ3dmVuIsnLn2CV6969bS/uOdtn0dieiLXN7/+lOtrMrIz2HBo\nA82qNjspEKaumcp939/HE5c+QY+IHqw/uJ7RC0fTqXYnhl00jMiGkZQPKk9ieiJ1wup47oc4HwsJ\nY3xIXJzrgsqUFNdIKC94srJO/jcjA9atg337TqzPP0o63SsgAOfEANcrsHwyAWFHKJ9Tj+ByudSo\nk8aF7cIIDHSFlIiyVN+mYeClNAq8hNBQ1xReZqbrLLeOHZXZez+hRkhdutS6nKBAP8qVc9UdEAA5\nubnsObabpuENCQlxjQB6NelF7bDaLIhZQHJmMh1rdeSTNZ+w6+guFu5ayLXNruVoxlFqlK/B1LVT\nGdB6AK9f/To/bP2Bu7+7m5cvf5kh7YeQmpVKxJsRBPkHcXHdi+lUqxMX1LqAZ+Y/Q+e6nflh6w90\nrN2RjYc2oqo0qdqEr/p9xbMLnuWzdZ8RHhpOZk4mt7W9jbnb5zK0w1AeuOgBrp12LQDXN78egHUH\n19G8anMuqXcJmTmZLNq9iC83fEnTqk3Jzs1m5oCZ7D66m31J+7hn1j0cSTtCs6rN2HtsL2HBYTSv\n1tx1ksOisfz36v+yePdi1h1cR/1K9Xn8ksdZGbeSWX/OYuGuheD84r622bX0iOjBSwtfOuXU7Izs\nDOJT46kdVvv4VNvOxJ0MmD6AZ7o/w/XNr0dE2Je0j+j90VzT9BrikuIYOHMg6w+up0OtDrzT+x3a\n12xvIWGMOUHVFSinC5GdO2HjRteIKCfHFVb5/01NdR0LCg52jZDWrDn51OnsbNf2vFGWiOuVmQm1\nakFYmGtkFBBwYqSUN1rKW5+dDVWqQM2aEBSajr+fv2uKyB/8/ZXAQDn+/qVZ79Oxbjsui+h6fN3e\ntK18su017msznMaVm+Lvr8Qkb6JxlcbUqVGOgAAlOTOZsOAwth7eyrR10+havytXNr4SESErJ4u5\n2+cStTOKnNwcOtTqwJbDW1geu5yQwBBaVmvJI5c8Qr2K9eg/vT+z/5xN4yqNOZpxlC9u+YKu9bvi\n7+fPsYxjHMs4xqq4VUzfOJ3qodVPe7PM5Mxk0rLSCA4IZtzycczeOpvB7Qbz/C/PEx4aTp2wOgT4\nBbAibgVB/kEcTDmIv/gfPx407KJhfLruU0ICQigXUI6UrBTCQ8OJT41HEB7u/DD3dHI9xvjl315m\nyd1LqFepnoWEMab05eS4jv2kprqCKi+E8o+WMjNdy/7+J25Vk57uCqi8kVXB92Vmuqb/EhJOrM+/\nPX/5rCzXLW/Cwk60IyjoxKgqf3D5+0NoqCusKlVyfZ03Bejnd2IZvxw0IJnqFSvRurUr6Arj53ci\nCPO/goIKXx8QcOI9qYG7SMtJYe+xveRqLhfUvIDaYbVRVXI05/ijk0MCQ5i7bS6VylUiLSsNfz9/\nujfoztYjWzmYcpDLGlx2vD1vLn2TjrU6Etko0kLCGGPypKe7gqJ8edcv4vzTdPlDJW/UdOSI6w4G\neaOp/COr/Mvx8a7nzeTmFv65OTmuUMr/yjt5wt1L1TmOlXSivad75YVLwVdQkGtqMC9U884CfOQR\n6NXrHL53kzHGeFu5cq5XnuBg18jiXJaayiln2xU8XnW6V0aG6/15oxQ/P9cUYPPmxW+bjSSMMaYM\n84XnSRhjjPFRFhLGGGPc8mhIiEg9EVkgIhtEZJ2IPFpImUEissZ5LRKRwh9EYIwxxus8PZLIBp5U\n1TZAF+AhEWlZoMwO4G+qegHwL+ADD7fpnFSSDy4/F1n/fFdZ7hvnQf+Ky6Mhoar7VTXaWU4GNgF1\nC5RZqqpHnS+XFtx+vijrP6jWP99VlvvGedC/4vLaMQkRaQh0AJadptg9wBxvtckYY8zpeeU6CRGp\nAMwAHnNGFIWV6QkMBS4rbLsxxhjv8/h1EiISAHwPzFHVQp9CLyLtgZnANaq63U0Zu0jCGGP+gnP6\nthwi8gkQr6pPutneAJgP3K6qSz3aGGOMMWfFoyEhIt2AhcA6QJ3XM0CEK9x0ooh8ANwM7AIEyFLV\nzh5rlDHGmCLzmdtyGGOM8T6fuOJaRK4Rkc0i8qeIPF3a7SkuEdnpXDy4WkSWO+uqiMg8EdkiInNF\npFJpt7OoROQjETkgImvzrXPbHxEZKSJbRWSTiPQqtYYXkZv+vSAie0XkD+d1Tb5tvta/Qi96LQv7\nsJC+PUIZ2n8iEiwiy5zfJRtE5GVKet+p6jn9coJsmzNFFQhEAy1Lu13F7NMOoEqBdf8GhjvLTwOv\nlHY7z6I/lzmnN689U3+A1sBq58y6hs6+ldLuw1/o3wvOhaIFy7bywf7VAjo4yxWALUDLsrAPT9O3\nsrT/Qp1//Z1rzbqV5L7zhZFEZ2Crqu5S1SzgC6BvaTeqmKSQUVxfYIqzPAW4sRTa9Zeo6iIgocBq\nd/25AfhCVbNVdSew1dnH5yw3/cPZjwX19cH+FXbRa72ysA/PcEFvWdl/qc5isPN7JaEk950vhERd\nYE++r/eWgauyFfhJRFaIyD3OupqqegDnBxuoUbpNLLYabvpTcH/G+vD+fFhEokXkw3zDeZ/uX76L\nXpee5mfSJ/tYyAW9ZWL/iYifiKwG9gNRqrqxJPedL4REWdRNVTsBfZz7WXV3giO/snZGQVnrz3tA\nY1Xt4PznfL20G1RchVz0WmZ+JgvpW5nZf6qaq6odndFfdxGJLMl95wshEQs0yPd1PWedz1LVfc6/\nh4D/OcO9AyJSE9cPdC3gYGm3s5jc9ScWqJ+vnE/uT1U9lO8pWB/kG7L7ZP+ci15nAFNV9VtndZnY\nh4X1raztP1x9Ogb8AFxUkvvOF0JiBdBURCJEJAgYCHxX2o36q0Qk1PmrBhEpD/RyriP5DrjTKfZ/\nwLenr+mcIwXmeN315ztgoIgEiUgjoCmwvBTae7ZO6p/zHy/PzcB6Z9lX+zcJ2FjgrghlZR+e0rey\nsv9EJDxvqkxEQoCrnAPTJbfvSvvIfBGP3l/jnJWwFRhR2u0pZl8aOWdorXbCYYSzvirws9PPeUDl\n0m7rWfRpGhAHZAC7nXtwVXHXH2Ckc1bFJqBXabf/L/bvE2Ctsy//58wB+2r/ugE5+X4u/3D+z7n9\nmfSVPp6mb2Vi/wHtnD6tBtYAf9cz/D452/7ZxXTGGGPc8oXpJmOMMaXEQsIYY4xbFhLGGGPcspAw\nxhjjloWEMcYYtywkjDHGuGUhYYwXiEgPEZlV2u0w5mxZSBjjPXZRkvE5FhLG5CMig52HuPwhIuOd\nO2wmich/RWS9iPwkItWcsh1EZIlzJ9GZ+W6P0MQpFy0iK53bHwCEich052EvU0u1o8YUkYWEMQ4R\naQncCnR17tKbCwwGQoHlqtrWeWb7C85bpgD/cO4kuj7f+s+Ad5z1XYF9zvoOwKPOg1+aiEjXUuqq\nMUUWUNoNMOYccgXQCVghIgKUAw44YfGVU+ZTYKaIVAQqOQ8kwgmMr5ybN9ZV1e9w3RstE1cA4QTN\nPufraOfJYL+XUl+NKRILCWNOEGCKqj570kqR5wqU03zlz0ZGvuUc+/9nfIFNNxlzwnygn4hU58TD\n5Bs4zw7u55QZDCxy7t1/RES6OetvB351HmizR0T6OnUEObdwNsYn2V8yxjhUdZOI/BOYJyJ+QCbw\nMJACdHZGFAec4xY49+l/3wmBHc4txHECY6KIvOTU0b+wj/Ni14z5y+xW4cacgYgkqWpYabfDmNJg\n003GnJn9JWXOWzaSMMYY45aNJIwxxrhlIWGMMcYtCwljjDFuWUgYY4xxy0LCGGOMWxYSxhhj3Pp/\ngJEprUQpAwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa08050ac10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEZCAYAAAC5AHPcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/JwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa080512450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Evaluation ####\n",
    "\n",
    "#loss_and_metrics = model.evaluate(x_test, y_test, sample_weight=w_test, batch_size=1000)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=1000)\n",
    "print('[INFO] loss and metrics: {0}'.format(loss_and_metrics))\n",
    "\n",
    "# Loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['regr_loss'])\n",
    "plt.plot(history.history['val_regr_loss'])\n",
    "plt.title('regr loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['discr_loss'])\n",
    "plt.plot(history.history['val_discr_loss'])\n",
    "plt.title('discr loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "## Mean Squared Error\n",
    "#plt.plot(history.history['regr_mean_squared_error'])\n",
    "#plt.plot(history.history['val_regr_mean_squared_error'])\n",
    "#plt.title('regr mse')\n",
    "#plt.ylabel('mse')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "## Mean Absolute Error\n",
    "#plt.plot(history.history['regr_mean_absolute_error'])\n",
    "#plt.plot(history.history['val_regr_mean_absolute_error'])\n",
    "#plt.title('regr mae')\n",
    "#plt.ylabel('mae')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n",
    "\n",
    "## Accuracy\n",
    "#plt.plot(history.history['discr_acc'])\n",
    "#plt.plot(history.history['val_discr_acc'])\n",
    "#plt.title('discr accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'validation'], loc='upper left')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Make plots ####\n",
    "\n",
    "from keras.models import load_model\n",
    "import ROOT\n",
    "\n",
    "# Load model\n",
    "model_file = 'model.h5'\n",
    "model_weights_file = 'model_weights.h5'\n",
    "\n",
    "loaded_model = load_model(model_file)\n",
    "loaded_model.load_weights(model_weights_file)\n",
    "\n",
    "# Set styles\n",
    "ROOT.gROOT.LoadMacro(\"tdrstyle.C\")\n",
    "ROOT.gROOT.ProcessLine(\"setTDRStyle();\")\n",
    "ROOT.gStyle.SetPalette(57)  # kBird\n",
    "ROOT.gStyle.SetMarkerStyle(1)\n",
    "ROOT.gStyle.SetEndErrorSize(0)\n",
    "ROOT.gStyle.SetPadGridX(True)\n",
    "ROOT.gStyle.SetPadGridY(True)\n",
    "\n",
    "nentries_test = x_test.shape[0]/10\n",
    "#nentries_test = 100000\n",
    "\n",
    "y_test_meas = predict(loaded_model, x_test[:nentries_test, :])\n",
    "\n",
    "y_adv_test = [np.zeros((x_adv_test.shape[0],1), dtype=np.float32), np.zeros((x_adv_test.shape[0],1), dtype=np.float32)]\n",
    "y_adv_test_meas = predict(loaded_model, x_adv_test)\n",
    "\n",
    "print y_test[:nentries_test]\n",
    "print y_test_meas\n",
    "print y_adv_test_meas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# For regression\n",
    "h1 = ROOT.TH1F(\"h1\", \"h1\", 300, -0.3, 0.3)\n",
    "h2a = ROOT.TH2F(\"h2a\", \"h2a\", 100, -0.5, 0.5, 300, -0.3, 0.3)\n",
    "h2b = ROOT.TH2F(\"h2b\", \"h2b\", 100, -0.5, 0.5, 300, -0.5, 0.5)\n",
    "h2c = ROOT.TH2F(\"h2c\", \"h2c\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "h2d = ROOT.TH2F(\"h2d\", \"h2d\", 100, -0.5, 0.5, 400, -2, 2)\n",
    "h2e = ROOT.TH2F(\"h2e\", \"h2e\", 100, 0., 0.5, 400, -2, 2)\n",
    "\n",
    "for i in xrange(nentries_test):\n",
    "  y_true = y_test[0][i]\n",
    "  y_meas = y_test_meas[0][i]\n",
    "  h1.Fill(y_meas - y_true)\n",
    "  h2a.Fill(y_true, y_meas - y_true) \n",
    "  h2b.Fill(y_true, y_meas)\n",
    "  h2c.Fill(y_true, (y_meas - y_true)/abs(y_true))\n",
    "  h2d.Fill(y_true, (abs(1.0/y_meas) - abs(1.0/y_true))/abs(1.0/y_true))\n",
    "  h2e.Fill(abs(y_true), (abs(1.0/y_meas) - abs(1.0/y_true))/abs(1.0/y_true))\n",
    "\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "h1.SetMarkerStyle(20)\n",
    "h1.Draw()\n",
    "c.Draw()\n",
    "print h1.GetEntries(), h1.GetMean(), h1.GetRMS()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2a.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2b.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2c.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2d.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "h2e.SetStats(0)\n",
    "h2e.SetTitle(\"\")\n",
    "h2e.GetXaxis().SetTitle(\"gen 1/p_{T} [1/GeV]\")\n",
    "h2e.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T}\")\n",
    "#h2e.GetYaxis().SetRangeUser(-1, 2)\n",
    "h2e.Draw(\"COLZ\")\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "\n",
    "hname = \"h2e\"\n",
    "h = h2e.Clone(\"h2e_clone\")\n",
    "#h.Draw(\"COLZ\")\n",
    "#gPad.Print(hname+\".png\")\n",
    "#h.RebinX(2)\n",
    "\n",
    "#h_pfx = h.ProfileX(hname+\"_pfx\", 1, -1, \"s\")\n",
    "#h_pfx.SetMaximum(1.2)\n",
    "#h_pfx.SetMinimum(-0.2)\n",
    "#h_pfx.Draw()\n",
    "#h_pfx.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "#gPad.Print(h_pfx.GetName()+\".png\")\n",
    "#\n",
    "\n",
    "if True:\n",
    "  # Apply gaussian fits\n",
    "  gr1 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2 = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr1_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  gr2_aspt = ROOT.TGraphAsymmErrors(h.GetNbinsX())\n",
    "  for i in xrange(h.GetNbinsX()):\n",
    "    h_py = h.ProjectionY(\"_py\", i+1, i+1)\n",
    "    if h_py.Integral() < 15:  continue\n",
    "    #r = h_py.Fit(\"gaus\", \"SNQ\")\n",
    "    r = h_py.Fit(\"gaus\", \"SNQ\", \"\", h_py.GetMean() - 0.04*8, h_py.GetMean() + 0.04*8)\n",
    "    mean, sigma, meanErr, sigmaErr = r.Parameter(1), r.Parameter(2), r.ParError(1), r.ParError(2)\n",
    "    gr1.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2.SetPoint(i, h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "    gr1_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), mean)\n",
    "    gr1_aspt.SetPointError(i, 0, 0, sigma, sigma)\n",
    "    gr2_aspt.SetPoint(i, 1.0/h.GetXaxis().GetBinCenter(i+1), sigma)\n",
    "    gr2_aspt.SetPointError(i, 0, 0, sigmaErr, sigmaErr)\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1.SetMarkerStyle(20)\n",
    "  gr1.Draw(\"p\")\n",
    "  #gr1.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetMaximum(1)\n",
    "  h_pfx.SetMinimum(0)\n",
    "  h_pfx.Draw()\n",
    "  gr2.SetMarkerStyle(20)\n",
    "  gr2.Draw(\"p\")\n",
    "  #gr2.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #\n",
    "  hname1 = hname\n",
    "  h_pfx = h.ProfileX(hname1+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} bias\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  h_pfx.Draw()\n",
    "  gr1_aspt.SetMarkerStyle(20)\n",
    "  gr1_aspt.Draw(\"p\")\n",
    "  #gr1_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "  #\n",
    "  hname2 = hname\n",
    "  h_pfx = h.ProfileX(hname2+\"_pfx\", 1, -1, \"s\")\n",
    "  h_pfx.Reset()\n",
    "  h_pfx.SetStats(0)\n",
    "  h_pfx.SetBins(50, 0, 50)\n",
    "  h_pfx.GetXaxis().SetTitle(\"gen p_{T} [GeV]\")\n",
    "  h_pfx.GetYaxis().SetTitle(\"#Delta(p_{T})/p_{T} resolution\")\n",
    "  h_pfx.SetMaximum(1.2)\n",
    "  h_pfx.SetMinimum(-0.2)\n",
    "  #h_pfx.SetMaximum(0.1)\n",
    "  #h_pfx.SetMinimum(-0.01)\n",
    "  h_pfx.Draw()\n",
    "  gr2_aspt.SetMarkerStyle(20)\n",
    "  gr2_aspt.Draw(\"p\")\n",
    "  #gr2_aspt.Fit(\"pol1\", \"\", \"\", 0.025, 0.2499)\n",
    "  #ROOT.gPad.SetLogx(1)\n",
    "  #ROOT.gPad.Print(h_pfx.GetName()+\".png\")\n",
    "  #ROOT.gPad.SetLogx(0)\n",
    "    \n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# For classification\n",
    "hh1a = ROOT.TH1F(\"hh1a\", \"hh1a\", 120, -0.1, 1.1)\n",
    "hh1b = ROOT.TH1F(\"hh1b\", \"hh1b\", 120, -0.1, 1.1)\n",
    "\n",
    "for i in xrange(nentries_test):\n",
    "  if y_test[1][i] != 100.:  # mask_value is set to 100\n",
    "    hh1a.Fill(y_test_meas[1][i])\n",
    "\n",
    "for i in xrange(y_adv_test[1].shape[0]):\n",
    "  hh1b.Fill(y_adv_test_meas[1][i])\n",
    "\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "hh1a.SetLineColor(632)  # kRed\n",
    "hh1a.Scale(1.0/hh1a.Integral())\n",
    "hh1a.Draw(\"hist\")\n",
    "hh1b.SetLineColor(1)  # kBlack\n",
    "hh1b.Scale(1.0/hh1b.Integral())\n",
    "hh1b.Draw(\"same hist\")\n",
    "c.Draw()\n",
    "c.SetLogy()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "try:\n",
    "  y_true = np.concatenate((y_test[1][:nentries_test], y_adv_test[1]))\n",
    "except ValueError:\n",
    "  y_true = np.concatenate((y_test[1][:nentries_test, np.newaxis], y_adv_test[1]))\n",
    "y_pred = np.concatenate((y_test_meas[1], y_adv_test_meas[1]))\n",
    "y_pred_0 = np.concatenate((y_test_meas[0], y_adv_test_meas[0]))\n",
    "\n",
    "mask = (y_true != 100.)  # mask_value is set to 100\n",
    "y_true = y_true[mask]\n",
    "y_pred = y_pred[mask]\n",
    "y_pred_0 = y_pred_0[mask]\n",
    "\n",
    "mask = np.abs(1.0/y_pred_0) > discr_pt_cut\n",
    "y_true = y_true[mask]\n",
    "y_pred = y_pred[mask]\n",
    "y_pred_0 = y_pred_0[mask]\n",
    "\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(y_true, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_xlabel('False positive rate')\n",
    "ax.set_ylabel('True positive rate')\n",
    "ax.set_title('ROC curve')\n",
    "ax.set_xlim([0.0,1.0])\n",
    "ax.set_ylim([0.9,1.0])\n",
    "\n",
    "idx = np.searchsorted(tpr, [0.9, 0.925, 0.95, 0.97, 0.98, 0.985, 0.99, 0.995])\n",
    "print tpr[idx]\n",
    "print fpr[idx]\n",
    "print thresh[idx]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Build trigger ####\n",
    "\n",
    "pt_bins = (-0.50, -0.333333, -0.25, -0.20, -0.15, -0.10, -0.05, 0.05, 0.10, 0.15, 0.20, 0.25, 0.333333, 0.50)\n",
    "def find_pt_bin(pt):\n",
    "  ipt = np.digitize((pt,), pt_bins[1:])[0]  # skip lowest edge\n",
    "  ipt = np.clip(ipt, 0, len(pt_bins)-2)\n",
    "  return ipt\n",
    "def emtf_road_quality(ipt):\n",
    "  best_ipt = find_pt_bin(0.)\n",
    "  return best_ipt - abs(ipt - best_ipt)\n",
    "\n",
    "class MyTriggerV1(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_trigger_pt(self, y_meas):\n",
    "    pt = np.abs(1.0/y_meas)\n",
    "    pt_clipped = np.clip(pt, 3., 60.)\n",
    "    pt = pt * (1.0 + (0.08813 + 0.009504 * pt_clipped) * 1.28155)  # erf(1.28155/sqrt(2)) = 0.8 [90% upper limit from -1 to -1]\n",
    "    return pt\n",
    "  \n",
    "  def pass_trigger(self, x, ndof, y_meas, y_discr):\n",
    "    trk_mode = 0\n",
    "    x_mode_vars = np.equal(x[nlayers*5+3:nlayers*5+8], 1)\n",
    "    for i, x_mode_var in enumerate(x_mode_vars):\n",
    "      if i == 0:\n",
    "        station = 1\n",
    "      else:\n",
    "        station = i\n",
    "      if x_mode_var:\n",
    "        trk_mode |= (1 << (4 - station))\n",
    "\n",
    "    ipt1 = (x[nlayers*5+0:nlayers*5+1] * 6 + 6).astype(np.int32)\n",
    "    ipt2 = find_pt_bin(y_meas)\n",
    "    quality1 = emtf_road_quality(ipt1)\n",
    "    quality2 = emtf_road_quality(ipt2)\n",
    "    \n",
    "    if trk_mode in (11,13,14,15) and quality2 <= (quality1+1):\n",
    "      if np.abs(1.0/y_meas) > discr_pt_cut:\n",
    "        if ndof <= 3:\n",
    "          #trigger = (y_discr > 0.5)\n",
    "          trigger = (y_discr > 0.8)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "        else:\n",
    "          trigger = (y_discr > 0.5393)\n",
    "          #trigger = (y_discr > 0.95)\n",
    "      else:\n",
    "        trigger = True\n",
    "    else:\n",
    "      trigger = False\n",
    "    return trigger\n",
    "\n",
    "\n",
    "class MyTriggerV2(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    pass\n",
    "\n",
    "  def get_trigger_pt(self, x, y_meas):\n",
    "    zone = int(x[(nlayers*5) + 1] * 5)\n",
    "    \n",
    "    pt = np.abs(1.0/y_meas)\n",
    "    pt_clipped = np.clip(pt, 3., 60.)\n",
    "    #pt = pt * (1.0 + (0.081 + 0.009 * pt_clipped) * 1.28155)  # erf(1.28155/sqrt(2)) = 0.8 [90% upper limit from -1 to -1]\n",
    "    #pt = pt * (1.0 + (0.080 + 0.0051 * pt_clipped) * 1.28155)  # erf(1.28155/sqrt(2)) = 0.8 [90% upper limit from -1 to -1]\n",
    "    #pt = pt * (1.0 + (0.18643468 + 0.00983759 * pt_clipped))\n",
    "    #pt = pt * (1.0 + (0.19061872 + 0.00897454 * pt_clipped))\n",
    "    #pt = pt * (1.0 + (0.21251148 + 0.00658309 * 0.97 * pt_clipped))\n",
    "    #pt = pt * (1.0 + (0.21540622 + 0.00588042 * 0.97 * pt_clipped))\n",
    "    #pt = pt * (1.0 + (0.23736955 + 0.00444597 * pt_clipped))\n",
    "    \n",
    "    sf =[[  0.00000000e+00,   2.49868810e-01,   9.67491604e-03],\n",
    "         [  1.00000000e+00,   2.02819258e-01,   3.44642927e-03],\n",
    "         [  2.00000000e+00,   1.34788156e-01,   3.97331361e-03],\n",
    "         [  3.00000000e+00,   1.34788156e-01,   3.97331361e-03],\n",
    "         [  4.00000000e+00,   2.05471098e-01,   7.20871985e-03],\n",
    "         [  5.00000000e+00,   2.05471098e-01,   7.20871985e-03]]\n",
    "    \n",
    "    sf =[[  0.00000000e+00,   2.63994873e-01,   9.85030923e-03],\n",
    "         [  1.00000000e+00,   1.79664418e-01,   5.61202876e-03],\n",
    "         [  2.00000000e+00,   1.62771225e-01,   2.96276459e-03],\n",
    "         [  3.00000000e+00,   1.62771225e-01,   2.96276459e-03],\n",
    "         [  4.00000000e+00,   1.40744388e-01,   6.65116590e-03],\n",
    "         [  5.00000000e+00,   1.40744388e-01,   6.65116590e-03]] \n",
    "\n",
    "    a, b = sf[zone][1], sf[zone][2]\n",
    "    pt = pt * (1.0 + (a + b * pt_clipped))\n",
    "    return pt\n",
    "  \n",
    "  def pass_trigger(self, x, ndof, y_meas, y_discr):\n",
    "    trk_mode = 0\n",
    "    x_mode_vars = np.equal(x[nlayers*5+3:nlayers*5+8], 1)\n",
    "    for i, x_mode_var in enumerate(x_mode_vars):\n",
    "      if i == 0:\n",
    "        station = 1\n",
    "      else:\n",
    "        station = i\n",
    "      if x_mode_var:\n",
    "        trk_mode |= (1 << (4 - station))\n",
    "\n",
    "    straightness = int(x[(nlayers*5) + 0] * 6) + 6\n",
    "    \n",
    "    ipt1 = straightness\n",
    "    ipt2 = find_pt_bin(y_meas)\n",
    "    quality1 = emtf_road_quality(ipt1)\n",
    "    quality2 = emtf_road_quality(ipt2)\n",
    "    \n",
    "    if trk_mode in (11,13,14,15) and quality2 <= (quality1+1):\n",
    "      if np.abs(1.0/y_meas) > discr_pt_cut:\n",
    "        if ndof <= 3:\n",
    "          #trigger = (y_discr > 0.8)\n",
    "          trigger = (y_discr > 0.996)\n",
    "        else:\n",
    "          #trigger = (y_discr > 0.5393)\n",
    "          trigger = (y_discr > 0.992)\n",
    "      else:\n",
    "        trigger = (y_discr >= 0.)  # True\n",
    "    else:\n",
    "      trigger = (y_discr < 0.)  # False\n",
    "    return trigger\n",
    "\n",
    "# ______________________________________________________________________________\n",
    "mytrigger = MyTriggerV2()\n",
    "\n",
    "from rootpy.plotting import Hist, Efficiency\n",
    "from math import sqrt\n",
    "histograms = {}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Make trigger efficiency ####\n",
    "\n",
    "eff_pt_bins = (0., 0.5, 1., 2., 3., 4., 5., 6., 8., 10., 12., 14., 16., 18., 20., 22., 24., 26., 28., 30., 35., 40., 45., 50., 60., 80., 120.)\n",
    "\n",
    "hname = \"x_eff_vs_genpt_denom\"\n",
    "h1c_denom = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt10\"\n",
    "h1c_numer_1 = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt20\"\n",
    "h1c_numer_2 = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt30\"\n",
    "h1c_numer_3 = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt40\"\n",
    "h1c_numer_4 = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "hname = \"x_eff_vs_genpt_l1pt50\"\n",
    "h1c_numer_5 = Hist(eff_pt_bins, name=hname, title=\"; gen p_{T} [GeV]\", type='F')\n",
    "\n",
    "h1c_data = []\n",
    "\n",
    "# Loop over events\n",
    "for x, ndof, y_meas, y_discr, y_true in zip(x_test, x_test, y_test_meas[0], y_test_meas[1], y_test[0]):\n",
    "\n",
    "  ndof = 4  #FIXME\n",
    "  \n",
    "  zone = int(x[(nlayers*5) + 1] * 5)\n",
    "  \n",
    "  trigger = mytrigger.pass_trigger(x, ndof, y_meas, y_discr)\n",
    "  \n",
    "  #pt = mytrigger.get_trigger_pt(y_meas)\n",
    "  pt = mytrigger.get_trigger_pt(x, y_meas)\n",
    "  pt_true = np.abs(1.0/y_true)\n",
    "  \n",
    "  h1c_denom.fill(pt_true)\n",
    "  if trigger and (pt > 10.):\n",
    "    h1c_numer_1.fill(pt_true)\n",
    "  if trigger and (pt > 20.):\n",
    "    h1c_numer_2.fill(pt_true)\n",
    "  if trigger and (pt > 30.):\n",
    "    h1c_numer_3.fill(pt_true)\n",
    "  if trigger and (pt > 40.):\n",
    "    h1c_numer_4.fill(pt_true)\n",
    "  if trigger and (pt > 50.):\n",
    "    h1c_numer_5.fill(pt_true)\n",
    "    \n",
    "  if trigger:\n",
    "    h1c_data.append((zone, np.asscalar(np.abs(1.0/y_true)), np.asscalar(np.abs(1.0/y_meas))))\n",
    "\n",
    "h1c_eff = Efficiency(h1c_numer_2, h1c_denom)\n",
    "h1c_eff.SetStatisticOption(0)  # kFCP\n",
    "h1c_eff.SetConfidenceLevel(0.682689492137)  # one sigma\n",
    "h1c_eff.SetMarkerStyle(1)\n",
    "h1c_eff.SetMarkerColor(800)  # kOrange\n",
    "h1c_eff.SetLineColor(800)  # kOrange\n",
    "h1c_eff.SetLineWidth(2)\n",
    "h1c_eff.SetDirectory(0)\n",
    "histograms['h1c_numer_1'] = h1c_numer_1\n",
    "histograms['h1c_numer_2'] = h1c_numer_2\n",
    "histograms['h1c_numer_3'] = h1c_numer_3\n",
    "histograms['h1c_numer_4'] = h1c_numer_4\n",
    "histograms['h1c_numer_5'] = h1c_numer_5\n",
    "histograms['h1c_denom'] = h1c_denom\n",
    "histograms['h1c_eff'] = h1c_eff\n",
    "\n",
    "for xx, pt in [(h1c_numer_1, 10.), (h1c_numer_2, 20.), (h1c_numer_3, 30.), (h1c_numer_4, 40.), (h1c_numer_5, 50.)]:\n",
    "  b = xx.FindBin(pt+0.5)\n",
    "  print pt, xx.GetBinContent(b)/h1c_denom.GetBinContent(b)\n",
    "\n",
    "# Add corrections\n",
    "if False:\n",
    "  nbinsx = h1c_eff.GetTotalHistogram().GetNbinsX()\n",
    "  corrections = [0.0, 0.0, 0.0, 2.5596844660416443e-05, 9.071900979369844e-05, -0.00025407866698875234, 3.9318943372533294e-05, 0.00046750609322471475, -0.0012868531325132167, -0.0017629378199139414, -0.003412967281340079, -1.859263302672609e-05, -0.0027697063960877566, -0.011912089953303617, -0.012565904385017701, -0.01502952024543791, -0.007091832957321742, -0.011625792518549893, -0.017156862745097978, -0.007722007722007707, -0.008966446308134701, -0.00857140366324638, -0.021269462895507463, -0.009598486441034226, -0.01485925792486198, -0.010991760558270336]\n",
    "  assert(len(corrections) == nbinsx)\n",
    "  for b in xrange(1,nbinsx+1):\n",
    "    old_eff = h1c_eff.GetEfficiency(b)\n",
    "    new_eff = old_eff + corrections[b-1]\n",
    "    h1c_eff.GetPassedHistogram().SetBinContent(b, h1c_eff.GetTotalHistogram().GetBinContent(b) * new_eff)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "infile1 = ROOT.TFile.Open(\"emtf_eff_vs_genpt_l1pt20.root\")\n",
    "frame = infile1.Get(\"emtf_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "h1a_eff = infile1.Get(\"emtf_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "h1b_eff = infile1.Get(\"emtf2023_eff_vs_genpt_l1pt20_denom_clone\")\n",
    "\n",
    "c = ROOT.TCanvas()\n",
    "frame.Draw()\n",
    "h1a_eff.Draw(\"same\")\n",
    "h1b_eff.Draw(\"same\")\n",
    "h1c_eff.Draw(\"same\")\n",
    "c.Draw()\n",
    "\n",
    "# Find corrections\n",
    "if False:\n",
    "  nbinsx = h1c_eff.GetTotalHistogram().GetNbinsX()\n",
    "  corrections = []\n",
    "  for b in xrange(1,nbinsx+1):\n",
    "    eff1 = h1b_eff.GetEfficiency(b)\n",
    "    eff2 = h1c_eff.GetEfficiency(b)\n",
    "    corr = eff1 - eff2\n",
    "    corrections.append(corr)\n",
    "  print corrections"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Find pT scale factor\n",
    "\n",
    "if True:\n",
    "  from sklearn import linear_model\n",
    "  \n",
    "  h1c_data = np.asarray(h1c_data)\n",
    "  \n",
    "  h1c_data_zone = h1c_data[:,0].astype(np.int32)\n",
    "  \n",
    "  fig, axs = plt.subplots(2, 3, figsize=(4*3,4*2), tight_layout=True)\n",
    "  myscales = []\n",
    "  \n",
    "  for zone in xrange(6):  # 6 zones\n",
    "    h1c_data_sf = []\n",
    "\n",
    "    #for pt in np.linspace(12, 32, 6):  # 4 GeV bin size\n",
    "    for pt in np.linspace(14, 30, 5):  # 4 GeV bin size\n",
    "      if zone == 4 or zone == 5:\n",
    "        sel = ((h1c_data_zone == 4) | (h1c_data_zone == 5))  & ((pt-2) <= h1c_data[:, 1]) & (h1c_data[:, 1] < (pt+2))\n",
    "      elif zone == 2 or zone == 3:\n",
    "        sel = ((h1c_data_zone == 2) | (h1c_data_zone == 3))  & ((pt-2) <= h1c_data[:, 1]) & (h1c_data[:, 1] < (pt+2))\n",
    "      else:\n",
    "        sel = (h1c_data_zone == zone) & ((pt-2) <= h1c_data[:, 1]) & (h1c_data[:, 1] < (pt+2))\n",
    "      #sel = ((pt-2) < h1c_data[:, 1]) & (h1c_data[:, 1] < (pt+2))\n",
    "      data = h1c_data[sel]\n",
    "      data = data[:,1] / data[:,2]  # pt_true/pt_xml\n",
    "      sf = np.percentile(data, [90.5], overwrite_input=True)\n",
    "      h1c_data_sf.append((pt, np.asscalar(sf)))\n",
    "    \n",
    "    # Fit\n",
    "    h1c_data_sf = np.asarray(h1c_data_sf)\n",
    "    h1c_x = h1c_data_sf[:,0][:, np.newaxis]\n",
    "    h1c_y = h1c_data_sf[:,1] - 1.0\n",
    "    linreg = linear_model.LinearRegression()\n",
    "    linreg.fit(h1c_x, h1c_y)\n",
    "    myscales.append((zone, linreg.intercept_, linreg.coef_[0]))\n",
    "    _ = axs[(zone/3, zone%3)].scatter(h1c_x, h1c_y)\n",
    "    _ = axs[(zone/3, zone%3)].plot(h1c_x, linreg.predict(h1c_x))\n",
    "\n",
    "  print np.array2string(np.array(myscales, dtype=np.float32), separator=', ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Make trigger rates ####\n",
    "\n",
    "hname = \"highest_x_absEtaMin0_absEtaMax2.5_qmin12_pt\"\n",
    "rates_hist = Hist(100, 0., 100., name=hname, title=\"; p_{T} [GeV]; entries\", type='F')\n",
    "\n",
    "rates_nevents = 2000\n",
    "rates_njobs = 100\n",
    "rates_array = np.zeros((rates_njobs,rates_nevents), dtype=np.float32)\n",
    "\n",
    "for x, ndof, y_meas, y_discr, aux in zip(x_adv_test, x_adv_test, y_adv_test_meas[0], y_adv_test_meas[1], aux_adv_test):\n",
    "  \n",
    "  ndof = 4  #FIXME\n",
    "  \n",
    "  (jobid, ievt, highest_part_pt, highest_track_pt) = aux\n",
    "  jobid = int(jobid)\n",
    "  ievt = int(ievt)\n",
    "  \n",
    "  #pt = mytrigger.get_trigger_pt(y_meas)\n",
    "  pt = mytrigger.get_trigger_pt(x, y_meas)\n",
    "  \n",
    "  trigger = mytrigger.pass_trigger(x, ndof, y_meas, y_discr)\n",
    "\n",
    "  if trigger:\n",
    "    rates_array[jobid,ievt] = max(rates_array[jobid,ievt], pt)\n",
    "\n",
    "rates_nevents_1 = 0\n",
    "\n",
    "for jobid in xrange(rates_array.shape[0]):\n",
    "  if rates_array[jobid].sum() > 0.:\n",
    "    for ievt in xrange(rates_array.shape[1]):\n",
    "      x = rates_array[jobid,ievt]\n",
    "      if x > 0.:\n",
    "        highest_pt = min(100.-1e-3, x)\n",
    "        rates_hist.fill(highest_pt)\n",
    "    rates_nevents_1 += rates_nevents\n",
    "\n",
    "print rates_nevents * rates_njobs, rates_nevents_1\n",
    "\n",
    "\n",
    "def make_ptcut(h):\n",
    "  use_overflow = True\n",
    "  binsum = 0\n",
    "  binerr2 = 0\n",
    "  for ib in xrange(h.GetNbinsX()+2-1, 0-1, -1):\n",
    "    if (not use_overflow) and (ib == 0 or ib == h.GetNbinsX()+1):\n",
    "      continue\n",
    "    binsum += h.GetBinContent(ib)\n",
    "    binerr2 += h.GetBinError(ib)**2\n",
    "    h.SetBinContent(ib, binsum)\n",
    "    h.SetBinError(ib, sqrt(binerr2))\n",
    "  return\n",
    "\n",
    "def make_rate(h, nevents):\n",
    "  orbitFreq = 11245.6\n",
    "  nCollBunches = 1866\n",
    "  nZeroBiasEvents = nevents\n",
    "  convFactorToHz = orbitFreq * nCollBunches / nZeroBiasEvents\n",
    "  h.Scale(convFactorToHz / 1000.)\n",
    "  return\n",
    "\n",
    "make_ptcut(rates_hist)\n",
    "make_rate(rates_hist, rates_nevents_1)\n",
    "\n",
    "rates_hist.SetLineColor(800)  # kOrange\n",
    "rates_hist.SetLineWidth(2)\n",
    "rates_hist.SetDirectory(0)\n",
    "histograms['rates_hist'] = rates_hist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "c = ROOT.TCanvas()\n",
    "rates_hist.Draw(\"hist\")\n",
    "c.SetLogy()\n",
    "c.Draw()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Rates ####\n",
    "\n",
    "infile2 = ROOT.TFile.Open(\"emtf2023_rate_reduction.root\")\n",
    "#cc1 = infile2.Get(\"cc1\")\n",
    "denom = infile2.Get(\"denom\")\n",
    "numer = infile2.Get(\"numer\")\n",
    "ratio = infile2.Get(\"ratio\")\n",
    "\n",
    "rates_hist_ratio = rates_hist.Clone(\"ratio2\")\n",
    "rates_hist_ratio.Divide(rates_hist_ratio, denom, 1, 1, \"\")\n",
    "\n",
    "cc1 = ROOT.TCanvas(\"cc1\", \"cc1\", 600, 700)\n",
    "cc1.Divide(1,2)\n",
    "cc1_1 = cc1.GetPad(1)\n",
    "cc1_1.SetPad(0.01,0.25,0.99,0.99)\n",
    "cc1_1.SetBottomMargin(0.01)\n",
    "cc1_1.SetGrid()\n",
    "cc1_1.SetLogy()\n",
    "cc1_2 = cc1.GetPad(2)\n",
    "cc1_2.SetPad(0.01,0.01,0.99,0.25)\n",
    "cc1_2.SetTopMargin(0.01)\n",
    "cc1_2.SetBottomMargin(0.43)\n",
    "cc1_2.SetGrid()\n",
    "\n",
    "cc1_1.cd()\n",
    "denom.Draw(\"hist\")\n",
    "numer.Draw(\"hist same\")\n",
    "rates_hist.Draw(\"hist same\")\n",
    "cc1_2.cd()\n",
    "ratio.Draw(\"hist same\")\n",
    "rates_hist_ratio.Draw(\"hist same\")\n",
    "cc1.Draw()\n",
    "\n",
    "print denom.GetBinContent(denom.FindBin(20.)), numer.GetBinContent(numer.FindBin(20.)), rates_hist.GetBinContent(rates_hist.FindBin(20.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from https://github.com/keras-team/keras/issues/4843\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.initializers import glorot_uniform, zero\n",
    "\n",
    "\n",
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "x = K.placeholder(name=\"x\", shape=(None, input_dim))\n",
    "ytrue = K.placeholder(name=\"y\", shape=(None, output_dim))\n",
    "\n",
    "hidden_dim = 128\n",
    "W1 = K.variable(glorot_uniform()([input_dim, hidden_dim]))\n",
    "b1 = K.variable(zero()((hidden_dim,)))\n",
    "W2 = K.variable(glorot_uniform()([hidden_dim, output_dim]))\n",
    "b2 = K.variable(zero()((output_dim,)))\n",
    "params = [W1, b1, W2, b2]\n",
    "\n",
    "\n",
    "hidden = K.sigmoid(K.dot(x, W1)+b1)\n",
    "ypred = K.softmax(K.dot(hidden, W2)+b2)\n",
    "\n",
    "\n",
    "loss = K.mean(K.categorical_crossentropy(ytrue, ypred),axis=None)\n",
    "\n",
    "accuracy = categorical_accuracy(ytrue, ypred)\n",
    "\n",
    "opt = Adam()\n",
    "updates = opt.get_updates(params, [], loss, )\n",
    "train = K.function([x, ytrue],[loss, accuracy],updates=updates)\n",
    "\n",
    "test = K.function([x, ytrue], [loss, accuracy])\n",
    "\n",
    "((xtrain, ytrain),(xtest, ytest)) = mnist.load_data()\n",
    "(xtrain, xtest) = [x.reshape((-1, input_dim))/255.0 for x in (xtrain, xtest)]\n",
    "(ytrain, ytest) = [to_categorical(y, output_dim) for y in (ytrain, ytest)]\n",
    "for epoch in range(1000):\n",
    "\tloss, accuracy = train([xtrain, ytrain])\n",
    "\ttest_loss, test_accuracy = test([xtest, ytest])\n",
    "\tprint(\"Epoch: {}, Train Loss: {}, Train Accuracy: {}, Test Loss: {}, Test Accuracy: {}\".format(\n",
    "\t\tepoch, loss, accuracy, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  print x_train.shape, y_train[0].shape, x_mask_train.shape\n",
    "  print np.mean(y_train[0]), np.std(y_train[0]), np.percentile(y_train[0], [2,98])\n",
    "  \n",
    "  fig, axs = plt.subplots(72/4, 4, figsize=(4*4,4*72/4), tight_layout=True)\n",
    "\n",
    "  for i in xrange(x_train.shape[1]):\n",
    "    lay = (i % nlayers)\n",
    "    mask = x_mask_train[...,lay].copy()\n",
    "    if i >= (nlayers*5):\n",
    "      mask *= False\n",
    "    \n",
    "    valid = ~mask\n",
    "    x_i = x_train[valid,i]\n",
    "    y_i = y_train[0][valid,0]\n",
    "\n",
    "    ymin, ymax = -0.6, 0.6\n",
    "    if i < (nlayers*3):\n",
    "      xmin, xmax = -1.5, 1.5\n",
    "    elif i < (nlayers*5):\n",
    "      xmin, xmax = -1.5, 1.5\n",
    "    elif i < 68:\n",
    "      xmin, xmax = -1.5, 1.5\n",
    "    \n",
    "    hist = axs[(i/4, i%4)].hist2d(x_i, y_i, bins=40, range=[[xmin, xmax], [ymin, ymax]], cmap=plt.cm.viridis)  #norm=colors.LogNorm(),\n",
    "    if x_i.size > 0:\n",
    "      print i, np.mean(x_i), np.std(x_i), np.percentile(x_i, [2,98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  from sklearn.linear_model import LinearRegression\n",
    "  from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "\n",
    "  #print x_train.shape, y_train[0].shape, x_mask_train.shape\n",
    "\n",
    "  #fig, axs = plt.subplots(72/4, 4, figsize=(4*4,4*72/4), tight_layout=True)\n",
    "  \n",
    "  coefs = np.ones((nlayers * 6) + 4)\n",
    "\n",
    "  for i in xrange(x_train.shape[1]):\n",
    "    lay = (i % nlayers)\n",
    "    mask = x_mask_train[...,lay].copy()\n",
    "    if i >= (nlayers*5):\n",
    "      mask *= False\n",
    "    \n",
    "    valid = ~mask\n",
    "    valid = valid & (np.abs(1.0/y_train[0]) < 14.)  # skip high pT part\n",
    "    x_i = x_train[valid,i].copy()\n",
    "    y_i = y_train[0][valid].copy()\n",
    "    \n",
    "    nentries_test = 100000\n",
    "    x_i = x_i[:nentries_test]\n",
    "    y_i = y_i[:nentries_test]\n",
    "    y_i /= (1.0/np.sqrt(12))  # (b-a)/sqrt(12)\n",
    "    \n",
    "    if x_i.size > 0 and np.std(x_i) > 0.:\n",
    "      coef = 1.0\n",
    "      \n",
    "      # x_phi\n",
    "      if (i < nlayers):\n",
    "        mcd = MinCovDet(assume_centered=True).fit(np.column_stack((x_i,y_i)))\n",
    "        coef = mcd.covariance_[0,1] / mcd.covariance_[0,0]  # Cov[x,y]/Var[x]\n",
    "        print i, coef, np.std(x_i), np.std(x_i * coef), np.std(y_i)\n",
    "        \n",
    "        #lr = LinearRegression(fit_intercept=False).fit(x_i[:,np.newaxis], y_i)\n",
    "        #coef = lr.coef_[0]\n",
    "        #print i, coef, np.std(x_i), np.std(x_i * coef), np.std(y_i)\n",
    "      \n",
    "      # x_theta\n",
    "      elif (nlayers) <= i < (nlayers*2):\n",
    "        if lay == 0 or lay == 1:  # ME1/1 or ME1/2\n",
    "          mcd = MinCovDet(assume_centered=True).fit(np.column_stack((np.abs(x_i),np.abs(y_i))))\n",
    "          coef = mcd.covariance_[0,1] / mcd.covariance_[0,0]  # Cov[x,y]/Var[x]\n",
    "        else:\n",
    "          coef = 1.0/np.std(x_i)\n",
    "        print i, coef, np.std(x_i), np.std(x_i * coef), np.std(y_i)\n",
    "      \n",
    "      # x_bend\n",
    "      elif (nlayers*2) <= i < (nlayers*3):\n",
    "        if lay == 0 or lay == 1:  # ME1/1 or ME1/2\n",
    "          mcd = MinCovDet(assume_centered=True).fit(np.column_stack((x_i,y_i)))\n",
    "          coef = mcd.covariance_[0,1] / mcd.covariance_[0,0]  # Cov[x,y]/Var[x]\n",
    "        else:\n",
    "          coef = 1.0/np.std(x_i)\n",
    "        print i, coef, np.std(x_i), np.std(x_i * coef), np.std(y_i)\n",
    "      \n",
    "      coefs[i] = coef  \n",
    "\n",
    "  print np.array2string(coefs, separator=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  def huber_loss(y_true, y_pred, delta=1.345):\n",
    "    x = K.abs(y_true - y_pred)\n",
    "    squared_loss = 0.5*K.square(x)\n",
    "    absolute_loss = delta * (x - 0.5*delta)\n",
    "    #xx = K.switch(x < delta, squared_loss, absolute_loss)\n",
    "    xx = tf.where(x < delta, squared_loss, absolute_loss)  # needed for tensorflow\n",
    "    #return K.mean(xx, axis=-1)\n",
    "    return xx\n",
    "\n",
    "\n",
    "  a = y_test[0][:nentries_test].copy()\n",
    "  b = y_test_meas[0].copy()\n",
    "\n",
    "  tmp = np.abs(1.0/a) > 20.\n",
    "  a = a[tmp]\n",
    "  b = b[tmp]\n",
    "\n",
    "  #reg_pt_scale = 14.\n",
    "  #a *= reg_pt_scale\n",
    "  #b *= reg_pt_scale\n",
    "\n",
    "  c = huber_loss(a, b)\n",
    "  sess = tf.InteractiveSession()\n",
    "  d = c.eval()\n",
    "  #sess.close()\n",
    "  e = (a-b).reshape(-1)\n",
    "  f = d.reshape(-1)\n",
    "\n",
    "  print len(a), len(b), a, b\n",
    "  print e, f\n",
    "  print np.min(f), np.max(f), np.median(f)\n",
    "  print 0.5 * np.square(e), 1.345 * e\n",
    "  print len(e), np.equal(0.5 * np.square(e), f).sum()\n",
    "  print np.std(e), np.median(np.abs(e))\n",
    "  \n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(e, bins=50, range=[-10,10])\n",
    "\n",
    "  #fig, ax = plt.subplots()\n",
    "  #ax.set_yscale('log')\n",
    "  #_ = ax.hist(0.5 * np.square(e), bins=50, range=[0,10])\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(f, bins=50, range=[0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  a = y_test[0][:nentries_test].copy()\n",
    "  b = y_test_meas[0].copy()\n",
    "\n",
    "  tmp = np.abs(1.0/a) > 20.\n",
    "  a = a[tmp]\n",
    "  b = b[tmp]\n",
    "\n",
    "  reg_pt_scale = 14.\n",
    "  a *= reg_pt_scale\n",
    "  b *= reg_pt_scale\n",
    "\n",
    "  c = huber_loss(a, b)\n",
    "  sess = tf.InteractiveSession()\n",
    "  d = c.eval()\n",
    "  #sess.close()\n",
    "  e = (a-b).reshape(-1)\n",
    "  f = d.reshape(-1)\n",
    "\n",
    "  print len(a), len(b), a, b\n",
    "  print e, f\n",
    "  print np.min(f), np.max(f), np.median(f)\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(e, bins=50, range=[-10,10])\n",
    "\n",
    "  #fig, ax = plt.subplots()\n",
    "  #ax.set_yscale('log')\n",
    "  #_ = ax.hist(0.5 * np.square(e), bins=50, range=[0,10])\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(f, bins=50, range=[0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  def binary_crossentropy(y_true, y_pred, from_logits=False):\n",
    "    target, output = tf.convert_to_tensor(y_true, np.float32), tf.convert_to_tensor(y_pred, np.float32)\n",
    "\n",
    "    # transform back to logits\n",
    "    if not from_logits:\n",
    "      output = K.clip(output, K.epsilon(), 1 - K.epsilon())\n",
    "      output = K.log(output / (1 - output))\n",
    "\n",
    "    xx =  tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n",
    "    #xx =  tf.nn.weighted_cross_entropy_with_logits(targets=target, logits=output, pos_weight=0.5)  # pos_weight < 1 decreases the false positive count\n",
    "    #return K.mean(xx, axis=-1)\n",
    "    return xx\n",
    "\n",
    "\n",
    "  a1 = y_test[1][:nentries_test].copy()\n",
    "  a2 = y_adv_test[1].copy()\n",
    "\n",
    "  b1 = y_test_meas[1].copy()\n",
    "  b2 = y_adv_test_meas[1].copy()\n",
    "\n",
    "  tmp = (a1 != 100.)\n",
    "  a1 = a1[tmp]\n",
    "  b1 = b1[tmp]\n",
    "\n",
    "  a2 = a2[:,0]\n",
    "  b2 = b2[:,0]\n",
    "  tmp = np.random.randint(0, len(a2), len(a1))\n",
    "  a2 = a2[tmp]\n",
    "  b2 = b2[tmp]\n",
    "\n",
    "  a = np.concatenate((a1, a2))\n",
    "  b = np.concatenate((b1, b2))\n",
    "\n",
    "  c = binary_crossentropy(a, b)\n",
    "  #sess = tf.InteractiveSession()\n",
    "  d = c.eval()\n",
    "  #sess.close()\n",
    "  e = (a-b).reshape(-1)\n",
    "  f = d.reshape(-1)\n",
    "\n",
    "  print len(a1), len(a2), len(b1), len(b2), a1, a2, b1, b2\n",
    "  print len(a), len(b), a, b\n",
    "  print np.min(f), np.max(f), np.median(f)\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(e, bins=50, range=[-10,10])\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_yscale('log')\n",
    "  _ = ax.hist(f, bins=50, range=[0,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_h5py():\n",
    "  import h5py\n",
    "\n",
    "  f = h5py.File(model_weights_file)\n",
    "  print f.keys()\n",
    "\n",
    "  keys = [u'dense_1', u'dense_2', u'dense_3', u'regr', u'discr']\n",
    "  for k in keys:\n",
    "    try:\n",
    "      w = f[k][k]['kernel:0'].value\n",
    "      b = f[k][k]['bias:0'].value\n",
    "      print k, w.shape, b.shape, np.min(np.abs(w)), np.max(np.abs(w))\n",
    "      \n",
    "      #FIXME\n",
    "      if k == 'dense_1':\n",
    "        a = np.sum(w*w, axis=0)\n",
    "        b = np.sort(a)/np.sum(a)\n",
    "        print \"..\", a, b\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "if False:\n",
    "  read_h5py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print('[INFO] Current time {0}'.format(str(datetime.now())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
